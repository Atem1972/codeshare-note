%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Welcome to the november 2023 session %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  week1 day1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
day 1
Intro to class orientation, q and a

best friends:
  1- youtube 
  2- google
  3- ChatGPT    https://chat.openai.com/chat
  
  NB: bookmark
  
*****week1 day2

Intro to computer basics hardwares and software 
1- BASIC COMPUTER HARDWARE

	a- What is a computer and how does it work? 
  
   Input ===> Process ====> Output
  
  b- Internal components (the case)
  	- Power supply
    - Motherboard
    - CPU
    - Heat sink
    - Hard drive (HDD, SSD)
    - RAM
    - Drive (CD, DVD)
    - Extension cards (sound, video, network )
  
  c- Input and output devices
  	- Input (Keyboard, mouse, webcam, microphone, scanner)
    - Output (monitor, printer, speakers)
  
2- BASIC COMPUTER SOFTWARE

a- What is a software and why do we need them?

b- Types of softwares
	- System softwares (OS = Windows, MacOS, Linux, Android, ios,...)
  		- Role/function of the OS in the computer architecture
      - Utility softwares intergrated in the OS (Antiviruses, disk/file managers, backup utilities, network utilities ...)
  - Application softwares (word processing, spreadsheets. web browsers, games, databases etc...)


** Create a lightsail server:

NB: always poke around, read the error, troubleshoot

** Linux commands

**cli ==> command line interface
Linux Flavors: centos, ubuntu Debian

date    ==> to check the date on the server
uptime  ==> to check how long the system have been running
clear   ==> the clear the terminal screen
free -m ==> check the total memory used and available memory
pwd     ==> print working directory ( check where you are on the server)
mkdir   ==> create folders or directories on the system
ls      ==> list directory content
touch   ==> create files 
history ==> check all previous typed commands
man     ==> manual of a command.
lsblk  ==> list block device ( hard drives)

%%%%%%%%%%%%%%%%%%%%%%% week2 day1 %%%%%%%%%%%%%%%%%%%%%%%%%%%

****intro to linux
  created in 91 by Linus Torvalds
  different flavor :
    centos            
    ubuntu
    debian
    kali
    mint
    alpine
    redhat
    fedora 
    and more ....
***linux components:
    Linux is composed of several key components that work together to provide the functionality of the operating system. 
These components include:
Kernel: The Linux kernel is the core of the operating system. It manages hardware resources, controls system processes, 
and provides services to applications.
Shell: The Linux shell is a command-line interface that allows users to interact with the operating system. 
It is a powerful tool that can be used to perform a wide range of tasks, from basic file management to complex system administration.
Filesystem: The Linux filesystem is a hierarchical structure that organizes files and directories. It is based on the Unix filesystem and provides a standardized way of accessing and storing data.
Applications: Linux supports a wide range of applications, including web browsers, office suites, media players, and development tools. Many applications are open source and can be freely downloaded and modified.
Libraries: Linux includes a large collection of libraries that provide common functionality to applications. These libraries can be used by developers to simplify the development process and make their applications more efficient.
Utilities: Linux includes a variety of utilities that provide additional functionality, such as text editors, network tools, and system monitoring tools. These utilities are often command-line based and can be used to automate tasks or perform complex operations.


***Linux commands practice:
  
date    ==> to check the date on the server
cal     ==> to check the calendar
uptime  ==> to check how long the system have been running
clear   ==> the clear the terminal screen
free -m ==> check the total memory used and available memory
free -g ==> same in gigabytes
pwd     ==> print working directory ( check where you are on the server)
mkdir   ==> create folders or directories on the system
ls      ==> list directory content
ls -l   ==> long listing
touch   ==> create files 
history ==> check all previous typed commands
man     ==> manual of a command. 
nproc   ==> check number of cpu
lsblk  ==> to check block device or hard drive
lscpu  ==> check cpu info 
cd     ==> change directory
cd ..  ==> move back one step
cd     ==> move back to your home directory
rm     ==> delete files
rm -r  ==> delete directory 
cat    ==> display the file content
vi,vim, nano     ==> edit files in linux 

*** System inventory ( server decommisioning , Server end of lease, end of life. )

	  uptime ==> to check how long the system have been running
    lscpu ==> check cpu info
    cat /etc/os-release  ==> os type and version
    nproc  ==> check number of cpu
    lsblk  ==> to check block device or hard drive
    uname -r ==> to check the kernel version
    top   ==> to check cpu, memory, uptime, and all the process running   
    free -m  ==> to check memory
    last reboot
    
*** files/directories commands
  	 mkdir  ==> to create folders
     touch  ==> to create files
     ll, ls, ls -l  ==> to list directory content
     cd  ==> to change directory
     cp, cp -r  ==> to copy files and directory 
     mv  ==> to rename a file or move it to a new location
     find ==> use to check file or directory path
     find / -name <file-name>
     vi  ==> to edit files
     cat ==> display file content
     rm -r, rm -rf  ==> to delete files and directories
    
*** package/softwares managment commands

yum install <package name>

example:  yum install httpd 
  
yum remove <package name>

example:  yum remove httpd
  
yum whatprovides <package or command>

example:  yum whatprovides git
  
*** Other commands:
    man
    history
    echo
  
Project1:
  1- You receive a request to create a test centos 7 server. go ahead and create it with lightsail service.
  with below specs:
   region: n. Virginia
    platform: linux
    blueprint: os only
    os :  centos 7
    keypair: N/A
    cpu: 2 vcpu's
    Memory: 512 MB
    hard drive: 15 
    Server Name: dev-test-server
      
  2- what is linux ?
  3- what are the features of linux 
  4- what is the linux architechture or  the components of linux?
  5- what is cli ?
  6- what is a shell?
  7- login the server created in question 1 and do a complete inventory
     - number of cpu's
     - cpu speed
     - kernel version
     - total memory size
     - hard drive number and size(s)
     - how long the system is up
     - what os is installed on it and the version
    
  8- create a file called linux_command
  9- add 30 linux commands in the file linux_command and what they are for.
  10- run a command to install git package on the server 
  10- what is your favorite command in linux and why?
 11- what are some linux flavor that you know ?
 12- delete the centos 7 server created in question 1.
 

 ******Project2 :
    
    1- At work , we need a server build. so we need to submit info about this server to the build team.
    we already have a server with the exact config needed.
    go ahead and login to the existing server, and get the info needed for the new server.
    existing server info:
      server name : 198.58.119.40
      username: u5bt
      password: abc
    
    we need below info:
      -number of hard drive:
      -hard drive size:
      -os or linux flavor and version:
      -kernel version:
      -Memory or ram total size in gigabytes:
      -number of cpu's:
      - how long the system have been running
      - number of users connected.
      
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week 3

1- board discuss traffic ==> LB ==> ws ==> DB ==> Monitoring ==> Third Party
                                          
*** different team at work 
    - network Team
    - security Team
    - infrastructure Team
    - middleware Team
    - linux SA Team
    - DBA Team
    - cloud Team
    - configuration Team
    - qa Team
    - uat Team
    - DevOps Team
    
2- Introduction to computer networks

	a- Overview on computer networks
  	- What is a computer network and why do we need it?
    - What are some network devices? (hub, switch, router)
    - The network can be wired (network cables) or wireless (Bluetooth, Wifi, ...)
  
  b- The OSI model (Open System Interconnect)
  	- What is the OSI model and why was it put in place?
    - What are the 7 layers of the OSI model?
    
  c- Difference betwee TCP and UDP protocols (transport Layer L4)
  
  d- The notion of IP address Version 4
  	- What is an IP address and how is it composed?
    - What are the classes of IP addresses?
    - What is a subnet mask
    - How to determine de number of IP addresses available in a network?
    - How to convert an IP address from octects to bits?
    - What is the difference between the IP address and the MAC address?
***connectivity:
  ping command
  
3- The vi command

	a- What is vi?
  
  b- Getting started with vi
  	- Open a file
    - vi modes (command and insert mode)
    - save and exit (ESC :wq)
    - save without exit (ESC :w)
    - exit without saving (ESC :q!)
    
  c- Useful tips in vi
  	- inserting text (i, o, a)
    - Deleting text (dd, dw, x, u)
    - copy, cut and paste (yy, dd, p)
    - move in file (arrow keys, k,j,h,l, [[, ]])
    - search text (/text)
 
4- **Linux commands review

        1 date ==> to check the date
        2 uptime ==> to check how long the server have been up
        3 whoami ==> to check who is logged in
        4 cd ==> to change directory
        5 cp ==> to copy files and directory
        6 mv ==> to rename a file or move it to a new location
        7 free ==> to check memory
        8 touch ==> to create files
        9 clear ==> to clear the terminal
       10 top ==> to check cpu, memory, uptime, and all the process running
       11 uname -r ==> to check the kernel version
       12 history ==> to check previous typed command
       13 exit ==> to exit a terminal or a user
       14 echo ==> to display any message
       15 sudo ==> to escalate privilege
       16 mkdir ==> to create folders
       17 lsblk ==> to check block device or hard drive
       18 cal ==> to check the calendar
       19 man ==> to check documentation about specific command
       20 ls ==> to list directory content
       21 pwd ==> to check the current working directory
       22 rm ==> to delete files and directories
       23 nproc ==> check number of cpu
       24 ssh ==> use to login to a linux server remotely 
       25 find ==> use to check file or directory path
       26 vi ==> to edit files
       27 id ==> to check user's id
       28 df ==> to check disk utilisation
       29 lscpu ==> check cpu info 
       30 cat ==> display file content
       31 more ==> display one page document at the time
       32 less ==> display one page document at the time
       33 grep ==> filter strings from file
       34 wget ==> download file from a url source
       35 yum, apt, ==> install packages 
       36 ping ==> check connectivity between two devices.
       37 systemctl ==> start, stop, restart, enable 
       36 cat /etc/os-release ==> os type and version 
   **Other notions 
       -pipe |  ==> use to separate many commands
       -redirect and append (> and >> )
      
  
  *** Project:
    
* Install apache

yum install httpd -y
systemctl start httpd
systemctl enable httpd
cd /var/www/html/
touch index.html
vi index.html
<h1> This is my first website my name is Serge </h1>


Project:

  1- what is the osi model?
  2- what are some problems found on l1 and l3
  3- what is the difference between TCP and UDP ?
  4- what is the difference between a hub and a switch?
  5- what happen when you type geico.com on the browser?
  6- what is an ip address ?
  7- what class is the ip 245.0.0.258 ?
  8- what do you use to edit files in linux ?
  9- how do you save and quit a file in vi?
  10- some packages are missing on the system and need to be installed type a command to install them.
  packages:  docker, java-11-openjdk-devel, net-tools, 
    
  11- if a server has an issue, and you need to pull just the lines in the log file with 
  the word error how can you do that?
  12- how do you create an empty file?
  13- how do you create an empty directory?
  15- what are some directories found in the / directory?
  16- create a file on your system called study.log
  17- in the file study.log , add 40 commands and what they are for ( dont cheat)
  18- type a command to display the content of study.log
  19- How can we display just the 5 last lines of study.log?
  20-there is a file that need to be downloaded on your server and this is the url to the file 
  please type a command to download that file.
  https://github.com/utrains/static-resume/archive/refs/heads/main.zip
  21- there is a file on your system called yum.log , type a command to display its content.
 
%%%%%%%%%%%%%%%% Project to do at home 
  
1- Apache is the most widely used webserver software in 
  the world. Developed and maintained by Apache Software Foundation, Apache is open source 
  software and available for free.
It’s fast, reliable, and secure. And Apache can be highly customized to meet the needs of 
many different environments by using extensions and modules.

Most WordPress hosting providers use Apache as their webserver software. However, 
WordPress can run on other webserver software as well.

  To install and configure apache webserver on a linux machine ( centos, amazon linux ) , we need to follow bellow steps
for centos:

**Install apache in centos/ redhat
yum install httpd -y 

** Start , check the status and enable apache daemon
systemctl start httpd
systemctl status httpd
systemctl enable httpd

** Create custorm content for the browser

cd /var/www/html
touch index.html
vi index.html 
<h1>This is my first website </h1>

go and refresh the browser

2-
a- create a linux server using the lightsail service in aws with below specs:
    region: n. Virginia
    blueprint: os only
    os :  centos 7
    key: week3
    cpu: 1
    Memory: 512 MB
      
b- gain root access and to a full inventory of the system:
      - total memory
      - number of cpu
      - os type and version
      - kernel version
      - number of hard drive and size
      - how long the system have been up
      
c- This server is going to be use to host an application and you are in charge of setting it up. there is a
    document in confluence on how to do it. follow the document and configure your server.
    confluence page link and credentials to log in:
      
      https://dataservicegroup.atlassian.net/l/cp/1BT00SB0
        
      Email: confluence-user@utrains.org
      Password : utrains-school123
  
d- navigate into the /var/www/html directory and in the index.html file  modify the name Alex Smith ( it is in 3 places in that file ) to your own name 
    then refresh the browser to see the changes. 
  
e- Take a backup or snapshot of the server so that we dont loose all our changes on the server when we delete it .
    give the name resume to the snapshot. 
  
f- go ahead and delete the server

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week 4 

 1- Users and groups in Linux
	a- Introduction: Why do we need users and groups on a Linux system
    
  b- Create groups (groupadd command, check with cat /etc/group or tail -3 /etc/group)
  
  c- Create users (useradd or adduser command, check with cat /etc/passwd )
  		- Useradd options: (-c, -g, -G, -m, -s, -D)
      - Fields of the /etc/passwd file
      	- Field 1: userame
        - Field 2: where password is stored (/etc/shadow)
        - Field 3: user account id
        - Field 4: group id
        - Field 5: user account description
        - Field 6: user account home directory
        - Field 7: Shell used
          
      - Types of user account on a system (system account, root account, regular user account)
  
  d- Delete users (userdel, userdel -r, userdel -f)
  
  e- Modify users (usermod with options -c, -l, -g, -G, -m, -s)
  
  f- Set user password (passwd)
  		- passwd command options (-l, -u, -d)

    
2- Files and directories permissions

	a- File and directory access (access denied for some users on some files)
  
  b- Check permissions on a file or directory
  		- Types of permissions (read=r=4, write=w=2, execute=x=1)
      - categories of users on which the permissions are applied (Owner=u, Group=g, Others=o)
  
  c- Modify permissions (chmod with letters or numbers)
  
  d- Change Ownership 
  	- change group (chgrp)
    - Change Owner (chown)
    - change Owner and Group (chown)
    
 -  rw-    r--  r--   . 1  root    root   0   Jan 16 01:47 review
    u      g    o
    6      4    4
    
chmod u+rw review  ==> this will add read and write permission to the owner
chmod u-rw review  ==> this will remove read and write permission to the owner

chmod 755 review   ==> this will give the owner full permission , the group and others read and execute
chmod 700 review   ==> this means owner full permission and group and other no permission
chmod 400 review   ==> owner read access and group, other nothing
chown henry review ==> change the owner of review to henry
chgrp cloud review ==> change the group that review belongs to, to cloud

project:
  
1- change the permission on class file to be 755
sol: 
  chmod 755 class
  
2-with the permission 755 what does that means for the file class

sol: full permission to the owner ,read and execute to group and others
  
3- change the permission on class as followed:
owner should have full permission,
group should have no permission 
other no permission

sol: 
  chmod 700 class
  
4- A file has permission 6 1 0 what does that mean?
owner: rw
group: x
other: 0


*** enable password authentication on aws server

vi /etc/ssh/sshd_config
:set nu
change PasswordAuthentication no to PasswordAuthentication yes 

systemctl restart sshd


 *****week4 day2

***************************Intro to cloud concept compare to on prem ************************

**What is Cloud Computing?
Cloud computing is the delivery of computing services over the internet which can mean faster 
innovation and flexible resources for your company. In very basic terms, cloud computing is 
when a business is able to operate part of or its entire company via the internet. 
This can include:

  - servers
  - storage
  - databases
  - networking
  - software

***Benefits of Cloud Computing

The following are the benefits of using cloud computing in your organization.

 - Cost: the cost of using cloud computing services is a lot cheaper than having to pay for the 
physical infrastructure needed for an office. Instead of paying for multiple computers, 
for example, each employee can have their personal computer and access everything needed from 
the internet.
 - Remote work: easy to have the whole company online, without the need to have an actual office. This is often preferable for many employees, and you can have workers from all over the world instead of only from one location. Related: Best Practices for Virtual Interviews
Innovation: with employees from all over the world you get a lot of different perspectives, 
which can increase the amount of creativity. Being able to have innovative solutions can break you apart from the competition which can lead to your business growing in size.
 - Scale: Businesses can grow easily when everything is done by cloud computing as it is easy 
to do things such as share resources or data needed to do the job. With costs down due to 
using a cloud computing system, you also have more monetary resources to get the help you 
need for certain projects, meaning your business can become bigger at a fast pace.
 - Collaboration: with cloud computing, collaborating becomes easy as there is a central 
system that allows employees to see the progress of their teammates. Communication between 
colleagues can become very easy as there are chat systems that allow teammates to communicate
directly no matter where they are located.
 - Data storage: it is a lot easier to store large amounts of data when you use cloud computing.
This can be very useful, especially for businesses that need to keep records for a particular 
amount of time or if your organization works with large data files such as high-definition photographs.
 
*** cloud Risks:
  
Although Cloud Computing is a great innovation in the world of computing, there also exist downsides of cloud
computing. Some of them are discussed below:
  
 - SECURITY & PRIVACY
It is the biggest concern about cloud computing. Since data management and 
infrastructure management in cloud
is provided by third-party, it is always a risk to handover the sensitive information 
to such providers.
Although the cloud computing vendors ensure more secure password protected accounts, any sign of security
breach would result in loss of clients and businesses.
  - LOCK-IN
  
It is very difficult for the customers to switch from one Cloud Service Provider (CSP) to another. It results in
dependency on a particular CSP for service.

  - ISOLATION FAILURE
This risk involves the failure of isolation mechanism that separates storage, memory, routing between the different
tenants

  - INSECURE OR INCOMPLETE DATA DELETION
It is possible that the data requested for deletion may not get deleted. It happens either because extra copies of
data are stored but are not available or disk destroyed also stores data from other tenants. 

**** Cloud Model:
  
- Public Cloud
- Private Cloud
- Hybrid Cloud
- Community Cloud
- Multi-Cloud 

**** cloud prividers:
  
as a whole, the top 10 cloud service providers globally in 2024 are Amazon Web 
Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), Alibaba Cloud, 
Oracle Cloud, IBM Cloud (Kyndryl), Tencent Cloud, OVHcloud, DigitalOcean, and 
Linode (owned by Akamai). 


*** Case study of aws:
  
* what is aws ?

Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud,
offering over 200 fully featured services from data centers globally. 
Millions of customers—including the fastest-growing startups, largest enterprises, 
and leading government agencies—are using AWS to lower costs, become more agile, 
and innovate faster.



  Project in groups:
    
    1- At work , we need a server build. so we need to submit info about this server to the build team.
    we already have a server with the exact config needed.
    go ahead and login to the existing server, and get the info needed for the new server.
    existing server info:
      server name : 198.58.119.40
      username: u5bt
      password: abc
    
    we need below info:
      -number of hard drive:
      -hard drive size:
      -os or linux flavor and version:
      -kernel version:
      -Memory or ram total size:
      -number of cpu's:
      -is it a physical or virtual machine: ?
      
    
    NB: below questions should be done on the same server 198.58.119.40 as well.
      
     2- run a command to check if the server is 64 bit or 32 bits
     3- there is a file on that server named cloud-init-output.log it in the /var/log directory
      who is the owner of this file?
      what is the permission on that file?
      what group does that file belongs to?
     4- the config managment team during a troubleshooting session have asked you provide them with
    the lines in the cloud-init-output.log file which has the word error. please go ahead and provide that.
     5- display the content of the file study-plan located in /opt/red/dev/study/utrains and read the content displayed.
     6- what group does study-plan belongs to?
     7- what is the permission on study-plan?
     8- whos is the owner of study-plan
    
  ********************Project to do at home***********************
  
  1- what is cloud computing?
  2- what are the advantages of cloud computing?
  3- what are some big players in the cloud computing landscape?
  4- what is aws ?
  5- what is a region in aws?
  6- what is aws cli?
  7- what is IAM in aws?
  8- what is route 53 ?
  9- what is vpc?
  10- what is s3 bucket?
  11- what is cloudwatch?
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week 5 Terraform
  
  *** main.tf
  
  terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  
  region = "us-east-1"
}

resource "aws_iam_group" "developers" {
  name = "developers"

}

resource "aws_iam_user" "lb" {
  name = "serge2026"
}
 
*** code3 exercice 


**** version.tf
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}
provider "aws" {

  region = "us-east-1"
}


**** lightsail.tf

resource "aws_lightsail_instance" "custom" {
  name              = "my-apache"
  availability_zone = "us-east-1b"
  blueprint_id      = "amazon_linux_2"
  bundle_id         = "nano_1_0"
  user_data         = "sudo yum install -y httpd && sudo systemctl start httpd && sudo systemctl enable httpd && echo '<h1>Deployed via Terraform</h1>' | sudo tee /var/www/html/index.html"
}

output "instance_ip" {
  value = aws_lightsail_instance.custom.public_ip_address
}
output "my-arn" {
  value = aws_lightsail_instance.custom.arn
}  

****** Project

1- in week4 folder write a terraform code that will create bellow ressources:
- a lightsail server with apache installed and the message "This is my first Terraform project " displayed 
on the browser when the server is created.
- a user called u5bt2024
- a group called cloudteam
2- we should have the public and private ip address of the lightsail server display at the end.
3- It is a good practice to modify the state file locally true or false ?
4- what are some blocks that you know in terraform.
5- there is a colleague that is having issue creating the lightsail server using terraform.
below is the code they have:

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}

resource "aws_lightsail_instance" "custom" {
  name              = "Dev server"
  availability_zone = "us-east-1b"
  blueprint_id      = "amazon_linux_2"
  bundle_id         = "nano_3_0"
  user_data         = "sudo yum install -y httpd && sudo systemctl start httpd && sudo systemctl enable httpd && echo '<h1>Deployed via Terraform</h1>' | sudo tee /var/www/html/index.html"
  tags ={
    Team = "DevOps"
    env  = "dev"
    create_by = "terraform"
  }
}

output "my-public-ip" {
  value = aws_lightsail_instance.custom.public_ip_address
}

output "My_username" {
  value = aws_lightsail_instance.custom.username
}

output "my_key" {
  value = aws_lightsail_instance.custom.key_pair_name
}  

Please create a folder called code5 and try to execute this code and see if you can help with the error.

6- after fixing the error in question5, we realised that the server doesnt have any key pair attach to it how can you solve that?


%%%%%%%%%%%%% Home project %%%%%%%%%%%%%%%%%%%%


***Terraform:

1. What is Terraform, and how does it relate to Infrastructure as Code (IAC)?
2. Explain the key benefits of using Terraform for infrastructure provisioning.
3. What is the purpose of the Terraform state file?
4. How do you initialize a Terraform configuration in a directory?
5. What are Terraform providers, and can you provide an example of some providers?
6. What is a resource in Terraform, and how is it defined?
7. what does terraform validate do? 
8. what is the output block in terraform?
9. what is the terraform block for? 
10. what does the terraform fmt command do?
11. What is the purpose of the `terraform plan` command?
12. What is the `terraform apply` command used for?
13. How do you destroy resources created by Terraform?
14. what is terraform module?
15. why is it not a good practice to modify the statefile manually?
16. everytime in the team, people need lightsail server and we need to create it manually in aws
this process is error prone and not easilly repeatable. propose a solution to automate this.
17- write a terraform code that will be used to create lightsail server as follow:

availability zone: us-east-1a
blueprint:  ubuntu_18_04
bundle : medium_1_0
name: dev server
user data: 
    #!/bin/bash
    sudo apt-get update
    sudo apt-get install -y apache2
    sudo systemctl start apache2
    sudo systemctl enable apache2
    echo '<h1>This is deployed by Serge </h1>' | sudo tee /var/www/html/index.html 
    
( change serge with your name)
we should be able to have the public and private ip display,

**** process managment:
1- what is the PID of a process?
2- what is an orphan process?
3- what is a zombie process?
4- how would you kill a process?
5- A server at work has a problem and you need to investigate.
login to this server and answer below question:

server info:
      server name : 198.58.119.40
      username: u5bt
      password: abc
  
a- check what user is running python3 program on the system? what is the first and last name of that user?
b- what is the process id of that process?
c- there is a program running on that server using port 8000 what program is that? who is running it ?
and what is the process id?
d- what transport protocol is configured on that port ?

cd ..
mkdir week6-clone 
286  cd week6-clone/
  287  git clone https://github.com/kserge2001/week6-git.git
  288  ls
  289  cd week6-git
  290  ls
  291  code version.tf
  292  code main.tf
  
  *** main.tf
  
  resource "aws_lightsail_instance" "server1" {
  name = "dev-server"
  blueprint_id = "ubuntu_18_04"
  bundle_id = "medium_1_0"
  availability_zone = "us-east-1a"
  user_data = <<-EOF
              #!/bin/bash
              sudo apt-get update
              sudo apt-get install -y apache2
              sudo systemctl start apache2
              sudo systemctl enable apache2
              echo '<h1>This is deployed by Serge </h1>' | sudo tee /var/www/html/index.html
              sudo useradd serge 
              EOF
}
  
*** output.tf

output "pip" {
  value = aws_lightsail_instance.server1.public_ip_address
}

output "privip" {
  value = aws_lightsail_instance.server1.private_ip_address
}

 git add .
git commit -m "lightsail"
git push origin ticket1

*** terraform default gitignore file

https://github.com/github/gitignore/blob/main/Terraform.gitignore
  

%%%%%%%%%%%%%%%%%%%%%% Projects to do at home

1- 
a- what is vcs? and why do companies need it ?
b- what is git ?
c- What is the difference between Git and GitHub
d- Explain the basic Git workflow?.
e- What is a Git repository?
f- How do you create a new Git branch?
g- What is a Git commit?
i- how do you solve a git push conflict?
j- list some git command you have used.
k- How do you undo the last Git commit?
l- What is a PR ( pull request) ?

2- on the repo week6-terraform-git in github, the region needs to be us-east-2 please go ahead and make the change
3- You receive a ticket to add terraform code on week6-terraform-git repository to create user called ansible 
and group call ansiblegroup
go ahead and do that on a branch called homeproject, push your changes to gihub, create a pull request 
and merge the changes to main branch.
4- give 4 reasons why every team should consider using a VCS
6- what is the difference between git and svn?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Bash shell script

sudo -i 
yum install -y git
git clone https://github.com/utrains/utrains_shell_script.git
cd utrains_shell_script
vi serge.sh
echo "hello world"
sleep 3
echo " I am learning bash script"

esc :wq
  
chmod +x serge.sh
./serge.sh


#!/bin/bash

echo "hello world"
sleep 3
echo " I am learning bash script"

**** system_inventory.sh

#!/bin/bash                            
                            
                            
# Description: system full inventory                            
# by Serge k , feb 2024                            
                            
                            
                            
echo "Below find the number of cpu:"
sleep 3
nproc                            
                            
echo "memory info " 
sleep 3
free -m                             
                            
echo "Kernel version below"
sleep 3
uname -r                             
echo "Below hard drive info"
sleep 3
lsblk

*** pkg.sh

#!/bin/bash

#Author : Utrains
    #Date : 01-Nov-2021

## ---------- script that Install some packages in Linux -----------------

echo "Install packages packages please wait ..."
sleep 3

yum install finger -y
yum install curl -y
yum install zip -y
yum install vim -y
  
****Project: Splunk is a software used for real time machine data processing.
  to install it , we follow bellow steps:
   
# Download the Splunk Enterprise tar file
cd  /opt
sudo yum -y install wget 
sudo wget -O splunk-9.0.4.1-419ad9369127-Linux-x86_64.tgz "https://download.splunk.com/products/splunk/releases/9.0.4.1/linux/splunk-9.0.4.1-419ad9369127-Linux-x86_64.tgz"

# Extract the tar file to /opt
sudo tar -zxvf splunk-9.0.4.1-419ad9369127-Linux-x86_64.tgz -C /opt

sudo rm -rf splunk-9.0.4.1-419ad9369127-Linux-x86_64.tgz

cd splunk/bin/
# Start Splunk Enterprise and set up the admin user and password
sudo ./splunk start --accept-license --answer-yes --no-prompt --seed-passwd "abcd1234"

#enable splunk at the startup
sudo ./splunk enable boot-start
# open splunk on the browser, thru port 8000 and login to check if everything works
# username= admin  and password = abcd1234 ( this can be change in the script.)

Go ahead and write a bash shell script to automate this process.

** return code 
after each command the special variable ? carries the exit code 
if the return/exit code is 0 then the command succeeded and if the exit code or return code is something else 
then it means the command failed.
example:
  ls   # successful command 
  echo $? will give 0 
  lscpiu  # failed command
  echo $?  will give 127 
  
  
**** game.sh

#!/bin/bash



winning_number=10

echo "welcome to 1xbet! good luck !!"
sleep 3

read -p "Please enter a choice between (1-1000)" USER_CHOICE


if [ $USER_CHOICE -eq  $winning_number ] 

then

echo "Congratulation you won!!!"

else 
echo "Sorry come back when you make more money"

fi

**** game.sh

#!/bin/bash



winning_number=10

echo "welcome to 1xbet! good luck !!"
sleep 3

read -p "Please enter a choice between (1-1000)" USER_CHOICE


if [ $USER_CHOICE -eq  $winning_number ]

then

echo "Congratulation you won!!!"

elif [ $USER_CHOICE -lt $winning_number ]

then
echo "Your choise is less than the winning number please try again"

elif [ $USER_CHOICE -gt $winning_number ]

then

echo "Please choose a lower number "

else
echo "Sorry come back when you make more money"

fi



*** Project:
  
  at work, we have requests for jenkins installation on centos 7. and to do it ,
  we follow a document. the problem with that is; errors and time wasted.
  we need to automate the process using a bash shell script. Go ahead and write the script for it
  Below are the steps followed for the installation:
    
sudo yum update -y

sudo yum -y wget 

sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo

sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

sudo yum upgrade -y

## install And Enable Docker
sudo yum install docker -y
sudo service docker start 
sudo systemctl enable docker.service

sudo chmod 777  /var/run/docker.sock


## install Git
sudo yum install git -y
yum install unzip -y

## Install Java 11:
#sudo amazon-linux-extras install java-openjdk11 -y

sudo yum install java-11* -y
## Install Jenkins then Enable the Jenkins service to start at boot :
sudo yum install jenkins -y
sudo systemctl enable jenkins

## Start Jenkins as a service:
sudo systemctl start jenkins

## Display Initial Jenkins Password
sudo cat /var/lib/jenkins/secrets/initialAdminPassword  
## Take ip to the browser and access it on port 8080 ( example: 198.40.2.78:8080 )

 %%%%%%%%%%%%%%%%%%%%%%%%%% week7 
  
  Terraform code
  
*** version.tf
terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  
  region = "us-east-1"
}

**** main.tf

resource "aws_instance" "server1" {
  ami = "ami-026b57f3c383c2eec"
  instance_type = "t2.micro"
  tags = {
    Name = "Terraform server"
    Team = "DevOps"
    env = "dev"
  }
  user_data = file("install.sh")
}

*** output.tf

output "public_ip" {
  value = aws_instance.server1.public_ip
}
output "az" {
  value = aws_instance.server1.availability_zone
}

*** install.sh

#!/bin/bash

sudo yum update -y
sudo yum install git httpd wget -y
sudo systemctl start httpd
sudo systemctl enable httpd
sudo groupadd DevOps
sudo useradd Serge

***main.tf ==> new


resource "aws_instance" "server1" {
  ami           = "ami-026b57f3c383c2eec"
  instance_type = "t2.micro"
  key_name = "week7key"
  tags = {
    Name = "Terraform server"
    Team = "DevOps"
    env  = "dev"
  }
  user_data = file("install.sh")
}

resource "aws_ebs_volume" "vol1" {
  availability_zone = aws_instance.server1.availability_zone
  size              = 10

  tags = {
    Name = "terraform volume"
    Team = "Cloud"
    created-by = "Terraform"
  }
}

resource "aws_volume_attachment" "ebs_att" {
  device_name = "/dev/sdh"
  volume_id   = aws_ebs_volume.vol1.id
  instance_id = aws_instance.server1.id
}


**** keypair.tf


# Generates a secure private and public key  and encodes it as PEM
resource "tls_private_key" "ec2_key" {
  algorithm = "RSA"
  rsa_bits  = 2048
}
# Create the Key Pair
resource "aws_key_pair" "ec2_key" {
  key_name   = "week7d2"
  public_key = tls_private_key.ec2_key.public_key_openssh
}
# Save file
resource "local_file" "ssh_key" {
  filename = "week7d2.pem"
  content  = tls_private_key.ec2_key.private_key_pem
}

**** new output.tf

output "public_ip" {
  value = aws_instance.server1.public_ip
}
output "az" {
  value = aws_instance.server1.availability_zone
}

output "ssh-command" {
  value = "ssh -i week7d2.pem ec2-user@${aws_instance.server1.public_ip}"
}

*** new install.sh

#!/bin/bash

sudo yum update -y
sudo yum install git httpd wget -y
sudo systemctl start httpd
sudo systemctl enable httpd
sudo groupadd DevOps
sudo useradd Serge
sudo yum install unzip  -y  #( apt install wget unzip -y )
wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip
unzip main.zip
cp -r static-resume-main/* /var/www/html/  

*** new main.tf

resource "aws_instance" "server1" {

  ami           = "ami-026b57f3c383c2eec"
  instance_type = "t2.micro"
  security_groups = [ "web" ]
  key_name      = "week7d2"
  tags = {
    Name = "Terraform server"
    Team = "DevOps"
    env  = "dev"
  }
  user_data = file("install.sh")
}

/*
resource "aws_ebs_volume" "vol1" {
  availability_zone = aws_instance.server1.availability_zone
  size              = 10

  tags = {
    Name       = "terraform volume"
    Team       = "Cloud"
    created-by = "Terraform"
  }
}

resource "aws_volume_attachment" "ebs_att" {
  device_name = "/dev/sdh"
  volume_id   = aws_ebs_volume.vol1.id
  instance_id = aws_instance.server1.id
}
*/


ssh to the server:
  
cd /var/www/html
ls
grep -in alex index.html   ==> this will display all the line with alex

sudo vim index.html 
:set nu   ==>  this is to set the line number in vi
change Alex Smith to your_name 
save and quit
refresh the browser


**** sg.tf

resource "aws_security_group" "sg-demo" {
    name = "web-sg"
    description = "Allow ssh and httpd"
    
    ingress {
        description = "allow ssh"
        from_port = 22
        to_port = 22
        protocol = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
    }
    ingress {
        description = "allow http"
        from_port = 80
        to_port = 80
        protocol = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
    }
    ingress {
        description = "allow http"
        from_port = 8080
        to_port = 8080
        protocol = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
    }
    egress {
        from_port = 0
        to_port = 0
        protocol = "-1"
        cidr_blocks = ["0.0.0.0/0"]
    }
  tags= {
    env = "Dev"
  }

  
}

********* Project:
  
    
You are working in the cloud infrastructure and configuration team and as such 
your team is working on migrating a multi tiers app fully hosted on aws.
A lot  have been done already in terms of inventory, planning , designing the architecture for aws,
and your team is now at the phase of creating the infrastructure in aws.
The project is broken down into user's stories or tickets. and these tickets are in the kanban board ( jira software):
Go ahead and use terraform to solve these tickets; dont hesitate to bring up 
any questions that you have during our morning stand up meetings.
                 
  
  
1- Create a vpc for utc app with below specs:
    
  - name: utc-app1
  - Tenancy: default 
  - NAT Gateway: 1 
  - s3 endpoint: no                            
  - CIDR: 172.120.0.0/16
  - enable IPV6: No                            
  - Enable DNS hostnames: yes
  - Enable DNS resolution: yes                            
  - Internet Gateway name: dev-wdp-IGW
  - Attach the Internet Gateway to the VPC
  - two public subnets
  - two private subnets                            
  - tags: {
    Name: utc-app1
    env:  dev
    team: wdp
    created by: Yourname
  }  
 
2- Create a security group for utc app:
    name: webserver-sg
    ports: 
      - 22 for ssh, allow just from your ip 
      - 80 for apache, open to the world
      - 8080 open everywhere
    vpc: utc-app1
      
3- create a ssh keypair for utc app :
    name: utc-key
    format: .pem
      
  
4- create an ec2 instance with below specs:
    ami: ami-0195204d5dce06d99
    hardrive / ebs: 20g 
    security group: webserver-sg
    enable public ip: yes
    key_name: utc-key
    vpc: utc-app1
    subnet: public-subnet-1a
    user data: 
   #!/bin/bash
   sudo  yum update -y
   sudo   groupadd docker
   sudo   useradd John -aG docker 
   sudo   yum install git unzip wget httpd -y
   sudo   systemctl start httpd
   sudo   systemctl enable httpd
   sudo   cd /opt
   sudo   wget https://github.com/kserge2001/web-consulting/archive/refs/heads/dev.zip
   sudo   unzip dev.zip
   sudo   cp -r /opt/web-consulting-dev/* /var/www/html
      
      tags = {
        Name: utc-dev-inst
        Team: Cloud Transformation
        Environment: Dev
        Created by: your name goes here
      }
      
5- testing:
  check the vpc and subnets
  check the keypair
  check the security group
  check the instance ( login and verify that user data run)
  Take the ip address of the instance to the browser to verify content

6- create an ami of your utc-dev-inst so next time we dont need to reconfigure the instance.
call it utc-dev-inst
  
7- clean-up
  terminate the instance
  delete security group
  delete keypair
  delete vpc
  
8- create an s3 bucket for the state file and a dynamodb table for the state file lock.
9- push the code to github.


%%%%%%%%%%%%%%%%%%%%%%%%%%%% week 8 Docker %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1- Introduction to virtualization

	a- What is virtualization (traditional server architecture vs virtualized server architecture)
  
  b- What is an Hypervisor?
  	- Hypervisor type 1
    - Hypervisor type 2
  
  c- Hypervisors vendors
   
    - Hypervisor type 1 vendors
      - VMware vSphere with ESX/ESXi
			- KVM (Kernel-Based Virtual Machine)
			- Microsoft Hyper-V
			- Oracle VM
			- Citrix Hypervisor (formerly known as Xen Server)
      
    - Hypervisor type 2 vendors
      - Oracle VM VirtualBox
      - VMware Workstation Pro/VMware Fusion
			- Windows Virtual PC
			- Parallels Desktop
      
  d- How to choose? (type 1 vs type 2)
  
2- Introduction to Docker part 1

- Open Container Initiative ( OCI)  standar of how container technology should be build.

-what is docker ?

-why docker?
  -little history before docker?
  -The big problem solved by docker

-what is docker Hub?

-sign up to docker hub,

-lab setup: 
  -terraform: https://dataservicegroup.atlassian.net/l/cp/4nBhbiVU

-some docker commands    
  -docker --version
  -docker info
  -docker images
  -docker ps
  -docker ps -a

** Images: what are those and where find them? ==>  docker hub

-docker pull
-docker images ubuntu
-docker run ubuntu 
-docker ps
-docker ps -a

**what are tags**

- running the ubuntu container in interactive mode
  -docker run -it [Image_id] bash
  -run some few linux commands then exit (pwd, ls, cat,exit)
  -docker ps
  -docker ps -a

- running container in detached mode 
  -docker run -itd image-id
  -docker ps

- running container with specific options
	-docker run -itd --name=giveAcontainername -p external:internal imageName
  -difference running with -it and -itd flags
  -docker exec command 
  -docker attach containerID (log into a specific container)

- login to docker 
   -docker login

- start/stop a container
		- docker stop container-id
    - docker start container-id
    - docker ps

-remove containers
    - docker rm containerID
    - docker rm containerID1  containerID2
    - docker rm -f containerID
    
- Delete images
    - docker rmi imageID
  
***** day2 Deploy Alex smith in container 

wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip
unzip main.zip
cp -r static-resume-main/* /usr/local/apache2/htdocs 

     1  pwd
    2  cd htdocs/
    3  ls
    4  pwd
    5  wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip
    6  apt update
    7  apt install wget vim unzip -y
    8  clear
    9  wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip
   10  history
   11  ls
   12  unzip main.zip
   13  ls
   14  ls
   15  cp -r static-resume-main/* .
   16  ls
   17  history
**** Dockerfile

FROM httpd

RUN apt update

RUN apt install wget unzip vim -y

EXPOSE 85
RUN wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip

RUN unzip main.zip

RUN cp -r static-resume-main/* /usr/local/apache2/htdocs 
    
*** build image

docker build -t kserge2001/alex-img .    # replace kserge2001 with your docker hub username)



*** Dockerfile

FROM httpd

RUN apt update

RUN apt install wget unzip vim -y

EXPOSE 85
RUN wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip

RUN unzip main.zip

RUN cp -r static-resume-main/* /usr/local/apache2/htdocs
RUN apt remove wget vim unzip -y
RUN rm -rf static-resume-main main.zip


78  vim Dockerfile
   79  clear
   80  docker build -t kserge2001/alex-img:2.0 .
   81  docker images
   82  docker build -t kserge2001/alex-img:2.0 .
   83  docker images
   84  docker push kserge2001/alex-img:2.0
   85  history
  
  docker rm -f $(docker ps -a -q)   # this will remove all containers
  docker rmi -f $(docker images -q) # this will remove all images
  
  
***** project

https://dataservicegroup.atlassian.net/wiki/spaces/LAT/pages/2219409409/Containerize+a+python+application
 

%%%% Docker Interview questions

1- what is docker?
2- why use docker?
3- what is a virtual machine?
4- what is the difference between virtual machine and a docker container ?
5- what is the difference between an image and a container?
6- what are some features of docker?
7- what is a dockerfile?
8- what are some instructions used in the dockerfile?
9- what is a docker registry?
10- what is docker compose? 
11- what is docker network? ( give example of networks )
12- How would make the data in the container persistent? 
13- How would you troubleshoot a container ?
14- what is the purpose of docker-compose.yaml?
15- what is docker swarm?
16- what are some best security practices when dealing with containers.?
17- what are some docker command you have used ?

%%%%%%%%%%%%%% week9

*** version.tf

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  
  region = "us-east-1"
}

*** Lamp configuration steps

https://dataservicegroup.atlassian.net/wiki/spaces/LAT/pages/2218459137/LAMP+Stack+installation+on+a+Linux+machine
****main.tf

# Generates a secure private k ey and encodes it as PEM
resource "tls_private_key" "lightsail_key" {
  algorithm = "RSA"
  rsa_bits  = 2048
}
# Create the Key Pair
resource "aws_lightsail_key_pair" "lightsail_key2" {
  name   = "lamp"
  public_key = tls_private_key.lightsail_key.public_key_openssh
}
# Save file
resource "local_file" "ssh_key" {
  filename = "lamp.pem"
  content  = tls_private_key.lightsail_key.private_key_pem
}

resource "aws_lightsail_instance" "server1" {
  name = "lamp-server"
  blueprint_id = "centos_7_2009_01"
  bundle_id = "medium_1_0"
  availability_zone = "us-east-1a"
  key_pair_name = "lamp"
}


*** output.tf

output "public" {
  value = aws_lightsail_instance.server1.public_ip_address
}
output "username" {
  value = aws_lightsail_instance.server1.username
}

output "ssh-command" {
  value = "ssh -i ${local_file.ssh_key.filename} ${aws_lightsail_instance.server1.username}@${aws_lightsail_instance.server1.public_ip_address}"
}

%%%%%%%%%%%% week 9d2

git clone https://github.com/utrains/docker-Lab.git
cd docker-Lab
terraform init
terraform plan
terraform apply --auto-approve

*** Docker Compose



** To make two container talk , we need to isolate them in the same network.
docker network ls
docker network create net 
docker run -d --rm --network net -e MARIADB_ROOT_PASSWORD=abc123 -e MARIADB_DATABASE=webserver -e MARIADB_USER=oracle -e MARIADB_PASSWORD=abc123 --name mariadb mariadb:latest
docker run -d --rm --name wordpress1 -p 80:80 --network net  -e WORDPRESS_DB_HOST=mariadb -e WORDPRESS_DB_USER=oracle -e WORDPRESS_DB_PASSWORD=abc123 -e WORDPRESS_DB_NAME=webserver wordpress
** It is stidious to manage container individually so best solution to run them is using docker-compose tool.

** Use docker compose 


version: '3'
services:
  wordpress:
    depends_on:
        - mysql
    image: wordpress:4.9.4-php7.2-apache
    #container_name: wordpress
    restart: always
    environment:
        - WORDPRESS_DB_USER=root
        - WORDPRESS_DB_PASSWORD=db_password
        - WORDPRESS_DB_HOST=mysql
        - WORDPRESS_DB_NAME=webserver
    ports:
        - 80:80
    networks:
        - web
    volumes:
        - wordpressdata:/var/www/html
  mysql:
    image: mariadb:10.4.12
    #container_name: db
    environment: 
        - MYSQL_ROOT_PASSWORD=db_password
        - MYSQL_USER=wp_user
        - MYSQL_PASSWORD=wp_password
        - MYSQL_DATABASE= webserver
    networks:
        - web
    command: '--default-authentication-plugin=mysql_native_password'
    volumes:
        - database:/var/lib/mysql
networks:
    web:
      driver: bridge
volumes:
    wordpressdata:
    database:

      
docker-compose up -d 
docker-compose ps

****route53.tf

resource "aws_route53_record" "rc1" {
  zone_id = "Z0889945UE3027Z54R69"
  type = "A"
  ttl = 300
  name = "resume.utrains.info"
  records = [ aws_lightsail_instance.server1.public_ip_address ]
}

*** resume.sh

#!/bin/bash

sudo yum update -y
sudo yum install git httpd wget -y
sudo systemctl start httpd
sudo systemctl enable httpd
# sudo groupadd DevOps
# sudo useradd Serge
sudo yum install unzip  -y  #( apt install wget unzip -y )
wget https://github.com/utrains/static-resume/archive/refs/heads/main.zip
unzip main.zip
cp -r static-resume-main/* /var/www/html/
exit(0)

*** output.tf
output "public" {
  value = aws_lightsail_instance.server1.public_ip_address
}
output "username" {
  value = aws_lightsail_instance.server1.username
}

output "ssh-command" {
  value = "ssh -i ${local_file.ssh_key.filename} ${aws_lightsail_instance.server1.username}@${aws_lightsail_instance.server1.public_ip_address}"
}
output "dns-name" {
  value = aws_route53_record.rc1.name
}


*** my github repo 
https://github.com/kserge2001/week9-lamp.git

**** Project A:


1- what is docker?
2- what is a virtual machine?
3- what is the difference between a virtual machine and docker ?
4- what is the difference between an image and a container?
5- what are some features of docker?
6- what is a dockerfile?
7- what is a docker registry?
8- what is docker compose? 
9- what id docker network?
10- How would make the data in the container persistent? 
11- How would you troubleshoot a container ?
12- what is the purpose of docker-compose.yaml?
13- what is docker swarm?
14- what are some best security practices when dealing with containers.?
15- what are some docker command you used ?  

*** Project B:
  
You are working in the cloud infrastructure and configuration team and as such 
your team is working on migrating a multi tiers app fully hosted on aws.
A lot  have been done already in terms of understanding the requirements, planning , designing the architecture for aws,
and your team is now at the phase of creating the infrastructure in aws.
The project is broken down into user's stories or tickets. and these tickets are in the kanban board ( jira software):
Go ahead and use terraform to solve these tickets; dont hesitate to bring up 
any questions that you have during our morning stand up meetings.
                 
  
  
1- Create a vpc for utc app with below specs:
    
  - name: utc-app1
  - Tenancy: default 
  - NAT Gateway: 1 
  - s3 endpoint: no                            
  - CIDR: 172.120.0.0/16
  - enable IPV6: No                            
  - Enable DNS hostnames: yes
  - Enable DNS resolution: yes                            
  - Internet Gateway name: dev-wdp-IGW
  - Attach the Internet Gateway to the VPC
  - two public subnets
  - two private subnets                            
  - tags: {
    Name: utc-app1
    env:  dev
    team: wdp
    created by: Yourname
  }  
 
2- Create a security group for utc app:
    name: webserver-sg
    ports: 
      - 22 for ssh, allow just from your ip 
      - 80 for apache, open to the world
      - 8000 open everywhere
    vpc: utc-app1
      
3- create a ssh keypair for utc app :
    name: utc-key
    format: .pem
      
  
4- create an ec2 instance with below specs:
    ami: ami-0195204d5dce06d99
    hardrive / ebs: 20g 
    security group: webserver-sg
    enable public ip: yes
    key_name: utc-key
    vpc: utc-app1
    subnet: public-subnet-1a
    user data: 
   #!/bin/bash
   sudo  yum update -y
   sudo   groupadd docker
   sudo   useradd John -aG docker 
   sudo   yum install git unzip wget httpd -y
   sudo   systemctl start httpd
   sudo   systemctl enable httpd
   sudo   cd /opt
   sudo   wget https://github.com/kserge2001/web-consulting/archive/refs/heads/dev.zip
   sudo   unzip dev.zip
   sudo   cp -r /opt/web-consulting-dev/* /var/www/html
      
      tags = {
        Name: utc-dev-inst
        Team: Cloud Transformation
        Environment: Dev
        Created by: your name goes here
      }
      
5- testing:
  check the vpc and subnets
  check the keypair
  check the security group
  check the instance ( login and verify that user data run)
  Take the ip address of the instance to the browser to verify content

6- create an ami of your utc-dev-inst so next time we dont need to reconfigure the instance.
call it utc-dev-inst for backup.
  
7- clean-up
  terminate the instance
  delete security group
  delete keypair
  delete vpc
8- This is on one project, and tomorrow or in the future, we will need to do the same and 
This manual process is very painfull!
For these reasons:
   
a. Human Error: Manual processes are prone to human error. Mistakes in configuration, 
installation, or updates can lead to system failures, security vulnerabilities, or data loss.

b. Inconsistency: 
Manual configurations can lead to inconsistencies across different environments 
(such as development, staging, and production). These inconsistencies can cause 
unexpected behavior when deploying applications or making changes.

c. Lack of Documentation: Manual changes often lack proper documentation, 
making it difficult for other team members to understand and replicate the configurations. 
This lack of documentation can hinder troubleshooting and future modifications.

d. Difficulty in Scaling: Manually managing infrastructure becomes increasingly challenging 
as the scale of the infrastructure grows. Managing a few servers manually might be feasible,
but as the number of servers, databases, and services increases, manual configurations become unmanageable.

e. Limited Visibility: Manual processes lack centralized visibility into the state of the 
infrastructure. Without proper monitoring and logging, identifying issues and troubleshooting
problems becomes time-consuming and challenging.

f. Security Risks: Manual processes might overlook security best practices, 
leading to vulnerabilities. Automated tools like Terraform can enforce security policies 
consistently, reducing the risk of misconfigurations.

g. Dependency Management: Manually managing dependencies between different components 
and services is complex and error-prone. Automated tools can handle dependency management 
more efficiently.

h. Slow Deployment: Manual processes often involve multiple steps and approvals, 
leading to slow deployment cycles. Automated solutions can significantly speed up the 
deployment process, allowing for faster iterations and updates.

i. Limited Rollback Capability: In case of failures during manual updates, rolling back 
to a previous stable state can be difficult and time-consuming. Automated tools often 
provide easy rollback mechanisms.

j. Compliance Challenges: Meeting regulatory and compliance requirements is harder with 
manual processes. Automated tools can enforce compliance policies consistently, 
ensuring that the infrastructure adheres to the necessary standards.

k. Difficulty in Reproducibility: It can be challenging to reproduce a specific configuration
or environment exactly as it was manually configured, leading to difficulties in testing, 
development, and troubleshooting.

What can you propose to mitigate these challenges and improve efficiency, reliability, 
and security in managing the infrastructure. ? 

9- create an s3 bucket for the state file and a dynamodb table for the state file lock.
10- push the code to github.
  

%%%%%%%%%%%%%%%%%%% week 10

** vpc.tf

resource "aws_vpc" "vpc1" {
 cidr_block = "192.168.0.0/16" 
 instance_tenancy = "default"
 tags = {
    Name = "Terraform-vpc"
    env = "dev"
    Team = "DevOps"
 }
}
resource "aws_internet_gateway" "gwy1" {
  vpc_id = aws_vpc.vpc1.id
}
# public subnet
resource "aws_subnet" "public1" {
    availability_zone = "us-east-1a"
    cidr_block = "192.168.1.0/24"
    map_public_ip_on_launch = true
    vpc_id = aws_vpc.vpc1.id
    tags={
        Name = "public-subnet-1"
        env = "dev"
    }
  
}
resource "aws_subnet" "public2" {
    availability_zone = "us-east-1b"
    cidr_block = "192.168.2.0/24"
    vpc_id = aws_vpc.vpc1.id
    map_public_ip_on_launch = true
    tags={
        Name = "public-subnet-2"
        env = "dev"
    }
  
}
#Private subnet

resource "aws_subnet" "private1" {
    availability_zone = "us-east-1a"
    cidr_block = "192.168.3.0/24"
    vpc_id = aws_vpc.vpc1.id
    tags={
        Name = "private-subnet-1"
        env = "dev"
    } 
}
resource "aws_subnet" "private2" {
    availability_zone = "us-east-1b"
    cidr_block = "192.168.4.0/24"
    vpc_id = aws_vpc.vpc1.id
    tags={
        Name = "private-subnet-2"
        env = "dev"
    }
  
}
#Elastic ip and Nat gateway
resource "aws_eip" "eip" {
  
}
resource "aws_nat_gateway" "nat1" {
  allocation_id = aws_eip.eip.id
  subnet_id = aws_subnet.public1.id
}
#Public route table

resource "aws_route_table" "rtpublic" {
 vpc_id = aws_vpc.vpc1.id 
 route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gwy1.id
 }
}

#Private route

resource "aws_route_table" "rtprivate" {
 vpc_id = aws_vpc.vpc1.id 
 route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_nat_gateway.nat1.id
 }
}

## Subnet association

resource "aws_route_table_association" "rta1" {
  subnet_id = aws_subnet.private1.id
  route_table_id = aws_route_table.rtprivate.id
}
resource "aws_route_table_association" "rta2" {
  subnet_id = aws_subnet.private2.id
  route_table_id = aws_route_table.rtprivate.id
}

resource "aws_route_table_association" "rta3" {
  subnet_id = aws_subnet.public1.id
  route_table_id = aws_route_table.rtpublic.id
}
resource "aws_route_table_association" "rta4" {
  subnet_id = aws_subnet.public2.id
  route_table_id = aws_route_table.rtpublic.id
}


*** backend.tf

terraform {
  backend "s3" {
    bucket         = "your bucket name"
    key            = "dev/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "your dynameDB table name"
  }
}

*** sg.tf

resource "aws_security_group" "sg1" {
    name = "Terraform-sg"
    description = "Allow ssh and httpd"
    vpc_id = aws_vpc.vpc1.id
    
    
    ingress {
        description = "allow http"
        from_port = 80
        to_port = 80
        protocol = "tcp"
        #cidr_blocks = ["0.0.0.0/0"]
        security_groups = [ aws_security_group.sg2.name ]
    }
    egress {
        from_port = 0
        to_port = 0
        protocol = "-1"
        cidr_blocks = ["0.0.0.0/0"]
    }
  tags= {
    env = "Dev"
    created-by-terraform = "yes"
  }

  
}
resource "aws_security_group" "sg2" {
    name = "Terraform-sg-lb"
    description = "Allow ssh and httpd"
    vpc_id = aws_vpc.vpc1.id
    
    ingress {
        description = "allow http"
        from_port = 80
        to_port = 80
        protocol = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
    }
    egress {
        from_port = 0
        to_port = 0
        protocol = "-1"
        cidr_blocks = ["0.0.0.0/0"]
    }
  tags= {
    env = "Dev"
  } 
}

*** ec2.tf

resource "aws_instance" "server1" {
  ami = "ami-02d7fd1c2af6eead0"
  instance_type = "t2.micro"
  vpc_security_group_ids = [ aws_security_group.sg1.id ]
  availability_zone = "us-east-1a"
  subnet_id = aws_subnet.private1.id
  user_data = file("code.sh")
  tags={
    Name = "webserver-1"
  }

}
resource "aws_instance" "server2" {
  ami = "ami-02d7fd1c2af6eead0"
  instance_type = "t2.micro"
  vpc_security_group_ids = [ aws_security_group.sg1.id ]
  availability_zone = "us-east-1b"
  subnet_id = aws_subnet.private2.id
  user_data = file("code.sh")
  tags={
    Name = "webserver-2"
  }

}
*** code.sh

#!/bin/bash

sudo yum update -y
sudo yum install httpd -y
sudo systemctl start httpd
sudo systemctl enable httpd
sudo bash -c 'echo "<html><body style=\"background-color: 47D34F;\"><h1><p> Welcome to Utrains the place to learn DevOps, Cloud, Linux!!.<br> This traf
fic is served from: <span style=\"color: purple;\">${HOSTNAME}</span> </p></h1></body></html>" > /var/www/html/index.html'

  *** alb.tf

resource "aws_lb_target_group" "alb-target-group" {
  name     = "application-lb-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.vpc1.id 

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 10
    matcher             = 200
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 6
    unhealthy_threshold = 3
  }
}
resource "aws_lb_target_group_attachment" "attach-app" {
  target_group_arn = aws_lb_target_group.alb-target-group.arn 
  target_id        = aws_instance.server1.id 
  port             = 80
}
resource "aws_lb_target_group_attachment" "attach-app" {
  target_group_arn = aws_lb_target_group.alb-target-group.arn 
  target_id        = aws_instance.server2.id 
  port             = 80
}
resource "aws_lb_listener" "alb-http-listener" {
    load_balancer_arn = aws_lb.application-lb.arn
    port              = "80"
    protocol          = "HTTP"
  
    default_action {
      type             = "forward"
      target_group_arn = aws_lb_target_group.alb-target-group.arn
    }
  }
resource "aws_lb" "application-lb" {
  name               = "application-lb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.sg2.id] 
  subnets            = [aws_subnet.public1.id, aws_subnet.public2.id]

  enable_deletion_protection = false

  tags = {
    Environment = "application-lb"
    Name        = "Application-lb"
    
  }
}

*** output.tf
output "dns-link" {
 value = aws_lb.application-lb.dns_name
}


%%%%% week10 day2

*** infracost setup  ( https://www.infracost.io/docs/ )

Infracost enables a shift-left approach for cloud costs by providing cost estimates for 
Terraform before deployment. Additionally, it can check for FinOps best practices in 
accordance with the Well-Architected Frameworks of cloud vendors, and your company's 
required tag keys/values. This not only saves your team money but also streamlines 
discussions about costs within the engineering workflow rather than it being a post-deployment 
consideration.

*** Mac
brew install infracost

*** Windows 
choco install infracost

** check ervion

infracost --version

*** set api token

infracost auth login

*** infrastructure cost 

infracost breakdown --path . 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week11

import boto3

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    response = s3.list_buckets()
    buckets = [bucket['Name'] for bucket in response['Buckets']]
    print("List of S3 buckets:")
    for bucket in buckets:
        print(bucket)

        
 

#!/bin/bash
sudo yum install wget httpd mysql php php-mysql -y
sudo wget https://wordpress.org/latest.tar.gz
sudo tar -xzf latest.tar.gz
sudo cp -r wordpress/* /var/www/html
sudo amazon-linux-extras install -y lamp-mariadb10.2-php7.2 php7.2
sudo cd /var/www/html
sudo cp /var/www/html/wp-config-sample.php /var/www/html/wp-config.php
sudo chown -R apache:apache /var/www/html
sudo systemctl start httpd 
sudo systemctl enable httpd


sudo vim /var/www/html/wp-config.php

db_name: webserver
db_username: admin
db_password: admin1234
db_host: <rds endpoint>

*************** Project

1- what is aws?
2- why cloud computing?
3- describe some of the things you have done in aws
4- how would you secure an app in aws?
5- describe the architechture of an 3 tier app in aws
6- what is serverless architechture?
7- what is the difference between ebs and s3 storage?
8- what is the security group?
9- what is vpc?
10- what is the difference between a private and a public vpc?
11- how do we represent the DMZ network in aws?
12- How would you automate the cloud infrastructure?
13- in a migration project to aws , what role do you play? ( week10 video)
14- what is terraform?
15- what is terraform remote backend?
16- go ahead an implement terraform in your team.

%%%%%%%%%%%%%%%%%%%%%%%%%% week12

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  
  region = "us-east-1"
}

data "aws_instance" "ec21" {
  instance_id = "i-0c9081dec94e3aa26"
}

resource "aws_instance" "demo" {
  ami = data.aws_instance.ec21.ami
  instance_type = data.aws_instance.ec21.instance_type
  key_name = data.aws_instance.ec21.key_name
}


*** data.tf

data "aws_ami" "ami1" {
  most_recent      = true
  owners           = ["amazon"]

  filter {
    name   = "name"
    values = ["amzn2-ami-kernel-5.10-hvm*-x86_64-ebs"]
  }

  filter {
    name   = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

terraform plan -generate-config-out=main.tf


**** main.tf

resource "aws_instance" "web" {
  ami                                  = "ami-033a1ebf088e56e81"
  associate_public_ip_address          = true
  availability_zone                    = "us-east-1b"
  instance_type                        = "t2.micro"
  ipv6_address_count                   = 0
  key_name                             = "wordpress"   # this should be the one generated in your screen
  monitoring                           = false
  security_groups                      = ["launch-wizard-1"]  # this should be the one generated in your screen
  subnet_id                            = "subnet-5594bd74"   # this should be the one generated in your screen
  tags = {
    Name = "webserver"
  }
  
  
}

**** day 2

meta arguments :
  * depend_on
  * count
  * lifecycle
  * for_each
  
  
  ****** count :
    - count is a meta-argument defined by the Terraform language. 
    - It can be used with modules and with every resource type.
    - The count meta-argument accepts a whole number, and creates that many instances of the resource or module.
    Exp : create 2 sames instance of an ec2
      
resource "aws_instance" "server" {
  count = 2 

  ami           = "ami-a1b2c3d4"
  instance_type = "t2.micro"

  tags = {
    Name = "Server ${count.index}"
  }
}

 ******** lifecycle : 
    - Lifecycle arguments help control the flow of your Terraform operations by creating custom rules for resource creation and destruction.
    - he arguments available within a lifecycle block are : 
      * create_before_destroy, 
      * prevent_destroy, 
      * ignore_changes, 
      * replace_triggered_by.
      Exp : 

resource "aws_instance" "server" {
  count = 2 

  ami           = "ami-a1b2c3d4"
  instance_type = "t2.micro"

	lifecycle {
    prevent_destroy = true
  }
 
}

Note : The prevent_destroy attribute is useful in situations where a change to an attribute would force a replacement and create downtime.
  
  ===========>  depend_on
	- Use the depends_on meta-argument to handle hidden resource or module dependencies that Terraform cannot automatically infer. 
	- You only need to explicitly specify a dependency when a resource or module relies on another resource's behavior but does not access any of that resource's data in its arguments.
 

*************  Terraform propvisioner 
- You can use provisioners to model specific actions on the local machine or on a remote machine in order to prepare servers or other infrastructure objects for service.
*remote-exec
* local-exec
* file

====> remote-exec  ==> execute command on target server
Example 1 : provision a web server in an aws_instance resource block, using remote-exec
===============================
connection {
   type        = "ssh"
   user        = "ec2-user"
   private_key = file("~/.ssh/id_rsa")
   host        = self.public_ip
   }
 
provisioner "remote-exec" {
   inline = [
     "sudo yum -y install httpd && sudo systemctl start httpd",
     "echo '<h1>My Test Website using Terraform Provisioner</h1>' > index.html",
     "sudo mv index.html /var/www/html/"
   ]
}



====> local-exec
Example 2 : create a local file called ip_address.txt, then put in it the ip address of an ec2 instance when this instance is created
===============================
provider "aws" {
  profile    = "default"
  region     = "us-west-2"
}

resource "aws_instance" "my_ec2" {
  ami           = "ami-06ffade19910cbfc0"
  instance_type = "t2.micro"

  provisioner "local-exec" {
    command = "echo ${aws_instance.my_ec2.public_ip} > ip_address.txt"
	}
  
}


====> file 
Example 2 : Copies the configs.d folder to /etc/configs.d
===============================
provisioner "file" {
source = "conf/configs.d"
destination = "/etc"
}

  
=====> null ressource  (empty resource container used when we do not want to create a specific resource.)
Example 1 : make ssh connexion, using null_resource and private_key
  

  
  
  resource "aws_instance" "server1" {
  ami = "ami-033a1ebf088e56e81"
  instance_type = "t3.small"
  key_name = "wordpress"
}

  
 terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  
  region = "us-east-1"
}

# Generated the secure key and encrypted to PEM format
resource "tls_private_key" "my_ec2_key" {
  algorithm = "RSA"
  rsa_bits = 2048
}
# Create aws key pair component in aws
resource "aws_key_pair" "ec2_key" {
  key_name = "week12key"
  public_key = tls_private_key.my_ec2_key.public_key_openssh
}
# Save my key pair file to current working directory
resource "local_file" "ssh_key" {
   filename = "${aws_key_pair.ec2_key.key_name}.pem"
   content = tls_private_key.my_ec2_key.private_key_pem
}

resource "aws_instance" "demo1" {
  ami = "ami-033a1ebf088e56e81"
  instance_type = "t2.micro"
  key_name = "week12key"

} 

**** main.tf

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}

provider "aws" {
  
  region = "us-east-1"
}

# Generated the secure key and encrypted to PEM format
resource "tls_private_key" "my_ec2_key" {
  algorithm = "RSA"
  rsa_bits = 2048
}
# Create aws key pair component in aws
resource "aws_key_pair" "ec2_key" {
  key_name = "week12key"
  public_key = tls_private_key.my_ec2_key.public_key_openssh
}
# Save my key pair file to current working directory
resource "local_file" "ssh_key" {
   filename = "${aws_key_pair.ec2_key.key_name}.pem"
   content = tls_private_key.my_ec2_key.private_key_pem
}

resource "aws_instance" "demo1" {
  ami = "ami-033a1ebf088e56e81"
  instance_type = "t2.micro"
  key_name = "week12key"
  
}

resource "null_resource" "n1" {
  connection {
    type  = "ssh"
    user = "ec2-user"
    private_key = file(local_file.ssh_key.filename)
    host = aws_instance.demo1.public_ip
  } 
  provisioner "local-exec" {
    command = "echo hello" 
  }
  provisioner "remote-exec" {
    inline = [
        "sudo useradd serge1",
        "mkdir terraform1",
    ]
  }
  provisioner "file" {
    source = "week12key.pem"
    destination = "/tmp/key.pem"
  }
 depends_on = [ aws_instance.demo1, local_file.ssh_key ]
}




*** vpc module

module "vpc" {
  source = "terraform-aws-modules/vpc/aws"

  name = "terraform-module"
  cidr = "10.0.0.0/16"

  azs             = ["us-east-1a", "us-east-1b"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]

  enable_nat_gateway = false
  enable_vpn_gateway = false

  tags = {
    Terraform = "true"
    Environment = "dev"
    Team = Devops
  }
}

************ Home project, Interview prep.

1- What is Terraform, and what problems does it solve?
2-How does Terraform differ from other infrastructure as code tools?
3-What are the main components of Terraform?
4-Explain the concept of Infrastructure as Code (IaC) and its benefits.
5-Describe the syntax used in Terraform configuration files.
6-What are Terraform providers? Can you give some examples?
7-How do you initialize a Terraform project?
8-What is Terraform statefile?
9-In your current company, how do you guys manage statefile?
10-How would use terraform to manage a ressource that was created outside of terraform?
11-What would you do if you loose your statefile
12-How do you manage sensitive data, such as passwords, in Terraform?
13-How do you handle dependencies between Terraform resources?
14- what is terraform module? and why is it important?
15- have you writen a module? if yes walk me thru it.
16-How do you handle Terraform configuration drift?
17-What is a Terraform graph, and how can it be useful?
18-What is the purpose of terraform init, terraform fmt, terraform validate, 
19- how would you check the cost of your aws infrastructure in terraform?
20-Describe an aws migration project you worked on and the role play in it.
21- what are some terraform blocks you know?
22- How would you delete just one resource in terraform?
23-How would you prevent accidental deletion of resource thru terraform destroy command.?
24-Write a terraform code that will do below:
- create an ec2 instance
- get the private ip address of the ec2 into a file called serverIp.log
- code the file serverIP.log into the ec2 instance in /opt directory.
25- what is a provisionner in terraform?
26- what is a null resource in terraform.
27- how would you refference a resource that was not created using terraform in your terraform code?
28- The team is adopting terraform as IAC tool. create a documentation on terraform best practices that will be used by the team.
29- as part of the adoption, you are in charge of writing few modules. go ahead and write a module for:
rds, sns, security group, ebs, ssh-key-pair for ec2, ssh-keypair for lightsail, lightsail.

NB: add them in the repo DevOps-Terraform-code-12 in the folder module.
  
30- Explain what this code is for :
  import {
  to = aws_instance.demo
  id = "i-0c2803efdc45be391"
}
31- for demo purpose, your manager needs an on demande solution:
he has a POC on docker and will need a solution that can be deployed for the demo and shutdown after the demo.
for now what is done is to lunch an ec2 instance, install docker and docker compose on it  manually and then terminate the instance
after the demo. please go ahead a propose a better or more automated solution.
32- what are the cons of terraform?

%%%%%%%%%%%%%%%%%%% week12 d4

provider "aws" {
    region = "us-east-1"
  
}

resource "aws_db_instance" "default" {
  allocated_storage    = 10
  db_name              = "mydb"
  engine               = "mysql"
  engine_version       = "5.7"
  instance_class       = "db.t3.micro"
  username             = "foo"
  password             = "foobarbaz"
  parameter_group_name = "default.mysql5.7"
  skip_final_snapshot  = true
  backup_retention_period = 0
  identifier = "dev-database"

}

** rds module

** main.tf
provider "aws" {
    region = var.region
  
}

resource "aws_db_instance" "default" {
  allocated_storage    = var.allocated_storage
  db_name              = var.db_name
  engine               = var.engine
  engine_version       = var.engine_version
  instance_class       = var.instance_class
  username             = var.username
  password             = var.password
  parameter_group_name = var.parameter_group_name
  skip_final_snapshot  = var.skip_final_snapshot
  backup_retention_period = var.backup_retention_period
  identifier = var.identifier
}

**variables.tf

variable "allocated_storage" {
 default = 10 
}
variable "backup_retention_period" {
 description = "How long backup should be kept" 
 default = 0
}
variable "db_name" {
  description = "Name of the initial database"
  default = "devdb"
}
variable "engine" {
  default = "mysql"
}
variable "engine_version" {
  default = "5.7"
}
variable "identifier" {
 default = "dev-database" 
}
variable "password" {
   sensitive = true 
  
}
variable "username" {
  sensitive = true
}
variable "instance_class" {
 default =  "db.t3.micro" 
}
variable "region" {
 default = "us-east-1"
}
variable "skip_final_snapshot" {
 default = true 
}
variable "parameter_group_name" {
 default = "default.mysql5.7" 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week13 DevOps Intro

aws sts get-caller-identity   #==> get user configured using aws configure command

**** DeVOps Concept *****

first what is SDLC ( software developement life cycle)
requirements==> Design==> Implementation or coding ==> verification/test ==> maintenance

method used:
  - Waterfall
  - Agile
  - Lean
  - DevOps
DevOps is a sdlc that emphasize on collaboration between ops and dev

DevOps is a combination of people, processes, and tools aimed at improving and 
automating the collaboration between development and operations teams. 
The key components of DevOps can be summarized as follows:

***People: DevOps emphasizes collaboration and communication between developers, operations 
staff, and other stakeholders. It encourages a culture of shared responsibility and 
continuous learning.

***Processes: DevOps promotes agile practices, continuous integration, continuous delivery 
(CI/CD), and other methodologies that enable faster and more reliable software delivery. 
It focuses on streamlining workflows, reducing bottlenecks, and improving efficiency across 
the software development lifecycle.

***Tools: Various tools are used in DevOps to automate processes, monitor performance, 
manage infrastructure, and facilitate collaboration. These tools can include version control 
systems (e.g., Git), continuous integration servers (e.g., Jenkins, GitLab CI/CD), 
configuration management tools (e.g., Ansible, Puppet, Chef), containerization and 
orchestration platforms (e.g., Docker, Kubernetes), and monitoring solutions (e.g., Prometheus, Grafana).

By integrating people, processes, and tools, DevOps aims to create a culture and environment where software development and IT operations teams can work together more effectively, deliver software faster, and maintain high levels of quality and reliability.

**DevOps Flow **
plan==>code==>build==>test <==> Integration <==>release==> deploy==> operate==> monitor 

***** Docker lab setup****** 

cd ~
mkdir week13
cd week13
git clone https://github.com/utrains/docker-Lab.git
cd docker-Lab
terraform init
terraform apply --auto-approve

aws ecr get-login-password --region us-east-1
docker run -d -p 8010:80 --name utcapp 076892551558.dkr.ecr.us-east-1.amazonaws.com/utc-app-dev:latest

221  cd static-app/
  222  ls
  223  rm -rf .git
  224  ls
  225  git init
  226  git status
  227  git add -A
  228  git status
  229  git commit -m "code"
  230  git remote add origin https://github.com/kserge2001/awscicd.git
  231  git branch -M main
  232  git push -u origin main
  233  history 
  
************************ Home project for interview prep ************************

1- How would you run container in the aws?
2- what have you done with docker?
3- what are some docker best practices?
4- what are the cons of docker?
5- What is the difference between monolithic and microservice application? which one is better and why?
6- Describe the SDLC lifecycle
7- are you familiar with agile method? please elaborate.
8- what do you think is DevOps?
9- what is continious integration?
10- what is the difference between continious delivery and continious deployment?
11- in your current company, how often do you deploy code to production?
12- I have a piece of code describe me the whole process how you will deploy it to prod.
13- what are the deploy strategies or patterns that you are familiar with? what have you used?
14- when a code or artifact is ready for production what happen in your company?
15- what is your experience building CI/CD pipeline?
16- The developer needs to inject a username=deploy-uer and a password=PasswOrd10! in ther code.
and it is not safe to add it in thier code. how can you help them with this issue in the pipeline?
17- How do you troubleshoot a failure in the CI/CD pipeline?
18- In the Dockerfile used in this project, (DockerfileProd) we can see that the base image httpd 
is comming from dockerhub; we need to get an image base on alipene, with some package installed on it 
and then stored in ecr. that image will then be used as a base image to avoid pulling image from docker hub.
Go ahead and make the necessary changes. packages { vim,curl,wget} port 8000 should be opened as well.
19-How would you add a stage in codepipeline?
20-In the utc-app-pipeline pipeline, add a step to create a container with the image built, display the container running before the image push.
this is just a test case to know if the image is good.
21-what is the difference between agile kanban,scrum and scrumban?

%%%%%%%%%%%%%%%%%%%%%%%%%%%% week 14 


 %%%%%%%%%%%%%%%%  Phone screening with the recruiter %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

*** recruiter intro ==> about the position, company and what he is looking for. ( focus on knowing you as a person)	
Tell me bit about you	
where are you located	
are you ready to relocate	
are you authorize to work in the US	
are you currently employed	
what is your company's technology stack 	
I see in your resume that you worked on ABC can you elaborate on that?	
any experience with JAVA?	
any python scripting ?	
are you ok with a hybrid position	
any azure experience? 	
why are you looking for a change?	
what is your expectation for your next role ( job description )	
what is your availability to start	
what are your expectation as far as compensation	( what is the pay range for this role? )

some tech questions given by the hiring manager:

1- what is a 3 way handshake?
2- what is a provider in terraform
3- how do download a docker image
4- what is permission 740
5- what is the port for ssh, http and https
6- how would you get all the lines with the word error in a file? 

Question you can ask: 

what does the interview process look like? how many rounds... any take-home project? 	
anything that I should be aware of ? i mean give me a tips to successfully pass the interview?	
when can I expect the next round to be scheduled?	

%%%%%%%%%%%%%%%%%%%    Hiring Manager     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Technical skills: ( can be learned by everybody )
*** The knowledge and understanding around the skilset in your RESUME.
Linux: 
Docker:
Scripting: 
CI/CD
Jenkins: 
Ansible:
eks:
IaC: ==>Terraform
Monitoring:
Production support?
Troubleshooting? ( thought process )

Cloud: 
CLI: 
  
Did you manage ?
Engineer those solutions? 
walk me through the process of  ... ( details explaination )
Problem solving skills:
scenario base questions: 

Cultural fit ( soft skill ):
  conflict:
  work under pressure:
  adapt to change 
  leadership 
  How do you relate to others?
  what will your colleague say about you?
  what do you think you have that is unique
  what would you do if you had super power
  How do you prioritise your work 

Communication and cross functional team collaboration:
  project done in collaboration

QUestions for hiring:
  get questions from what was discussed:
  Understand the team culture  
  Understand different technologies used in the team
  Understand expectation in the next 15 to 30 day
  Hiring manager profile
  
%%%%%%%%%%%%%%%%%%%%%%%%% Technical interview %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Listen carefully to the questions:
  ask for clarification if needed
Build your answer:
  take time to respond
Know how to not explain what you dont know.
and do your homework ( build these solutions in preparation)
  
  
  %%%%%%%%%%%%%%%%%%%%%%% week15 
  
  https://github.com/kserge2001/geo.git
 
%%%%%%%%%% week 16

****** Projects

1- what is DevOps ?
2- why should we care about DevOps?
3- who is a good DevOps engineer?
4- what is CI ( continious integration ) ?
5- what is shift left concept?
6- what is the difference between continious delivery and continious deployment
7- explain the ci/cd pipeline concept 
8- what are some deployment patterns you are familiar with?
9- how often do you deploy to production?
10- How do you get work assign to you?
11- describe a typical day at work
12- what is an artifactory ? which one are you familiar with?
13- there is new app that is been developped. the app is going to be used by patients and health provider to 
schedule appointments, track treatements, and locate providers nearby.
this app is developped in java.
this is the link to the code base: https://github.com/kserge2001/geo.git
after a meeting with the dev team, they said after writing or making a change to the code,
they run series of commands to compile, build, test and package the code for qa ( or more lower env) testing.
mvn clean
mvn install
mvn compile
mvn test
mvn package
mvn sonar:sonar
  
Your job is to build the CI/CD pipeline for this project.
the goal is to set up the CI pipeline as follow:
  every time the developer make a change and push it , a job will trigger to automate all what the developer was doing manually.
  the jar file generated should be store in jfrog each time the pipeline runs.

14- are you familiar with the agile framework?
15- what is jenkins? 
16- how do you create jobs in jenkins?
17- what is a plugin in jenkins?
18- give an example of automation you have done using jenkins.
19- what is the difference between scripted and declarative pipeline?
20- how would you scale jenkins?
21- how would you protect jenkins?
22- how would you take a backup of jenkins server
23- how do you handle automatic job trigger in jenkins?
24- if I have a script to take backup, can I use jenkins to schedule the script to execute everyday at 12:00?
  if yes how ?
25-how do you fix failed jobs in jenkins?


*** Devops strategy 
https://dataservicegroup.atlassian.net/l/cp/ZFNz9pv2

  
*** Jenkinsfile

pipeline{
    agent any

    stages{
        stage('init'){
            steps{
                sh 'terraform init -no-color'
            }
        }
        stage('validate'){
            steps{
                sh 'terraform validate -no-color'
            }
        }
        stage('format'){
            steps{
                sh 'terraform fmt -no-color'
            }
        }
        stage('plan'){
            steps{
                sh 'terraform plan -no-color'
            }
        }
    }
}


****** DevOps Project:
  
1- An app is been actively developped and will be used by medical providers and patient to manage, schedule
appointments, treaments, suggestions in health method and more.
for now the developer are compiling, packaging the code locally and also copying the artifacts to 
directly to the qa server.
problems they face: 
  - slow build
  - manual code analytic using sonarqube
  - tools configuration locally
  - no proper versioning of code
  - code stored on dev laptop
  - no integrity of the artifact copy to qa server
  - a lot of back and forth and troubleshooting.
  - lack of history and visibility
  - many more 
The app team have organised a meeting to mitigate some or all of these issues and you are part of the meeting to work on helping with automation and best practices for a smooth deployment to prod.
Go ahead and propose a solution.

NB: use the geo app repo for this.

2- the code discussed in question is in this repo <https://github.com/kserge2001/geo.git>
After some discussion with developer, we can see that they are running some maven command locally like:

mvn clean
mvn validate
mvn compile
mvn install
mvn package
mvn test
then a jar file is generated and stored in the target folder. which they use scp command to copy it to the qa server.
go ahead and build a solid and automated CI/CD pipeline to help speed up this process. include 
the code scan steps in your pipeline.

3- we need an ubuntu server as agent in jenkins please go ahead and set that up.
that ubuntu 20.04 server should have docker, java ( version 17), python3 installed.

4- There is a new app been develop and the app is develop in nodejs language. for the discussion around
deploying the app and all the setup, you are on the call. your job is the setup the fully automated 
CI/CD pipeline. go ahead and do that. repo url <https://github.com/utrains/nodeLogin.git>
NB: The app owner is ready for any questions
  
5- Create a document that the qa team can use to deploy the jar file stored in artifactory on question 2
please make sure you test this.
  
6-The difference in environment is bringing a lot of issues and troubleshooting during deployment and we need a solution. what can you propose?
go ahead and implement your suggestion.


*** alias gt='git add .;git commit -m "project"; git push origin main'
tar -cvf jenkins_backup.tar  *

curl -uadmin:AP8gcgmmset5jeYChTJYDN6XmDd -T <PATH_TO_FILE> "http://ec2-54-161-239-95.compute-1.amazonaws.com:8081/artifactory/geolocation/<TARGET_FILE_PATH>"


***** steps

# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl -y
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

sudo apt install openjdk-17-jdk
sudo apt install python3

*** user data for qa

#!/bin/bash
sudo yum update -y
sudo yum install java-17* -y
sudo install wget curl -y

## Configure Tomcat 

cd /opt
wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.9/bin/apache-tomcat-10.1.9.tar.gz
# unzip tomcat binary
tar -xvzf apache-tomcat-10.1.9 
# Rename for simplicity 
mv apache-tomcat-10.1.9 tomcat
## Node setup

curl -sL https://rpm.nodesource.com/setup_14.x | sudo bash -
sudo yum install -y nodejs

*** Deployment steps

#### QA deployment steps for utc app 

1- download artifact 

curl -uadmin:AP8gcgmmset5jeYChTJYDN6XmDd -O "http://52.91.134.51:8081/artifactory/utc-nodejs/<TARGET_FILE_PATH>"

# Replace this <TARGET_FILE_PATH> with your artifact

2- Bring up the app

java -jar <artifact>   # replace <artifact> with the downloaded file

3- Access app on port 8082 

4- Kill the app

ctrl + C

5- clean up

rm -rf *.jar

%%%%%%%%%%%%%%%%%%%%%%%%%% week 17

Jenkins_ami: ami-0651a24cc46a968a0
Jefrog_ami:  ami-0aca76f2fc36ba481

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week18

Ansible 

play1.yml

---
- hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Install nginx
      apt:
        name: nginx
        state: present
        update_cache: yes
      when:  ansible_os_family=="Debian"
    - name: Install nginx on redhat family
      yum:
        name: nginx
        state: present
      when: ansible_os_family=="RedHat"
    - name: Create user serge
      user:
        name: Serge
        comment: Serge Smith
        shell: /bin/bash
    - name: create group
      group:
        name: cloudteam
        state: present
    - name: create index file
      file:
        path: /var/www/html/index.html
        state: touch
    - name: Create directory
      command: mkdir /tmp/os


*** New playbook

---
- hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Install nginx
      apt:
        name: nginx
        state: present
        update_cache: yes
      when:  ansible_os_family=="Debian"
    - name: Install nginx on redhat family
      package:
        name: httpd
        state: present
      when: ansible_os_family=="RedHat"
    - name: Create user serge
      user:
        name: Serge
        comment: Serge Smith
        shell: /bin/bash
    - name: create group
      group:
        name: cloudteam
        state: present
    - name: create index file
      file:
        path: /var/www/html/index.html
        state: touch
    - name: Create directory
      file:
        path: /tmp/os
        state: directory


***** new file play1.yml

---
- hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Install nginx
      apt:
        name: nginx
        state: present
        update_cache: yes
      when:  ansible_os_family=="Debian"
    - name: Install nginx on redhat family
      package:
        name: httpd
        state: present
      when: ansible_os_family=="RedHat"
    - name: Start httpd daemon
      service: 
        name: httpd
        state: started
        enabled: yes
      when: ansible_os_family=="RedHat"
    - name: Create user serge
      user:
        name: Serge
        comment: Serge Smith
        shell: /bin/bash
    - name: create group
      group:
        name: cloudteam
        state: present
    - name: create index file
      file:
        path: /var/www/html/index.html
        state: touch
    - name: Create directory
      file:
        path: /tmp/os
        state: directory
    
    - name : Copy content to index
      copy:
        dest: /var/www/html/index.html
        content: '<h1>This is created using ansible</h1>'


**** play2.yaml

---
- hosts: ws
  become: 1
  gather_facts: 1
  ignore_errors: True
  vars:
     user_name: serge2026
     group: terraform 

  tasks:
    - name: check server uptime
      command: uptime
    - name: list directory
      shell: ls
    - name: check the kernel version
      command: uname -r
    - name: create user
      user:
        name: "{{user_name}}"
    - name: create group
      group:
        name: "{{group}}"

*** variables.yml

my_command: uptime
pkg: httpd

*** play2.yaml

---
---
- hosts: ws
  become: 1
  gather_facts: 1
  ignore_errors: True
  vars:
     user_name: serge2026
     group: terraform 
  vars_files:
    - variables.yml
  tasks:
    - name: check server uptime
      command: "{{my_command}}"
      register: my_uptime
    - name: list directory
      shell: ls
    - name: check the kernel version
      command: uname -r
      register: kernel_output
    - name: create user
      user:
        name: "{{user_name}}"
    - name: create group
      group:
        name: "{{group}}"
    - name: Read data 
      debug:
        var: my_uptime.stdout[:19]
          
          
 **** play1.yml

---
- hosts: all
  become: yes
  gather_facts: yes
  tasks:
    - name: Install nginx
      apt:
        name: nginx
        state: present
        update_cache: yes
      tags:
        - apt_get
      when:  ansible_os_family=="Debian"
    - name: Install nginx on redhat family
      package:
        name: httpd
        state: present
      when: ansible_os_family=="RedHat"
    - name: Start httpd daemon
      service: 
        name: httpd
        state: started
        enabled: yes
      when: ansible_os_family=="RedHat"
    - name: Create user serge
      user:
        name: Serge
        comment: Serge Smith
        shell: /bin/bash
    - name: create group
      group:
        name: cloudteam
        state: present
    - name: create index file
      file:
        path: /var/www/html/index.html
        state: touch
      register: file_output
    - name: Create directory
      file:
        path: /tmp/os
        state: directory
    
    - name : Copy content to index
      copy:
        dest: file_output.diff.before.path
        content: '<h1>This is created using ansible</h1>'


 %%%%%%%%%%%%%%%% Ansible projects:
     
     1- As servers are created using terraform there is a need to track configuration done on them in an 
     automated way. Ansible is brought to the team as a configuration management tool for the job.
     
     various playbooks , roles ,plugins, collections, modules and more will be developped.
  ***Problem:   There is a problem of tracking who is adding what or changing what in those ansible codes/playbooks . 
     Propose a best practice to help track changes and reviews those changes before we deploy it to the 
     ansible server. ( github ) 
     
     2- create a pipeline in jenkins that will help deploy the ansible playbook automatically create a zip file 
     with all the code , store it in jfrog and deliver those files to the ansible server on 
     the path /home/ec2-user/ansible-dev
    
   sol:
    
   *** jenkinsfile

pipeline{
    agent any

    stages{
        stage('zip the file'){
            steps{
                sh 'rm -rf *.zip || echo ""'
                sh 'zip -r ansible-${BUILD_ID}.zip * --exclude Jenkinsfile'
            }
        }
        stage('upload artifacts to jfrog'){
            steps{
                sh 'curl -uadmin:AP8gcgmmset5jeYChTJYDN6XmDd -T \
                ansible-${BUILD_ID}.zip \
                "http://34.201.161.49:8081/artifactory/ansible/ansible-${BUILD_ID}.zip"'
            }
        }
        stage('publish to ansible server'){
            steps{
                sshPublisher(publishers: [sshPublisherDesc(configName: 'AnsibleServer', \
                transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: \
                'unzip -o ansible-${BUILD_ID}.zip; rm -rf ansible-${BUILD_ID}.zip', execTimeout: 120000, flatten: false, makeEmptyDirs: false, \
                noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: \
                '.', remoteDirectorySDF: false, removePrefix: '', \
                sourceFiles: 'ansible-${BUILD_ID}.zip')], usePromotionTimestamp: false, \
                useWorkspaceInPromotion: false, verbose: false)])
            }
        }
    }
}

    
     3- every month, we need to run yum update -y or apt update -y on all the servers.
     we need to automate this task to run every month automatically.
     4- we need jenkins to execute the playbook instead of someone login into the ansible server and execute those playbook
     How can we make that happen?
     5- Each time a qa server is deployed using terraform, we need to configure the server. it is done thru
     a shell script and you need to change it to  use a playbook
     go ahead and write the playbook  for that. below is the script used.
     #!/bin/bash
     yum install lsof wget passwd docker unzip -y
     yum install java-11* -y
     mkdir -p /opt/qa/spg
     touch  /opt/qa/spg/spg.log
     systemctl start docker
     systemctl enable docker 
     
     6- these are the steps used by the qa team on the qa server built in question 5 to deploy the spring boot app ( java jar file)
     
     - Delete old artifact:  rm -rf /opt/qa/spg/*.jar
     - Check if an app is running on port 8082: lsof -i :8082  ( identify the process id and kill it )
     - download the new artifact from nexus to /opt/qa/spg folder: wget --username admin --passowrd devops http://198.58.119.40:8081/repository/prof-repo/bioMedical-0.0.6-SNAPSHOT.jar  ( the version of artifact can change )
     - Bring up the app in the background: nohup java -jar /opt/qa/spg/*.jar > /opt/qa/spg.log 2>&1 &
     Go ahead and write a playbook to automate this manual process.
     
     7- meeting with monitoring team to understand the process and set up automations for it.
     
   
  %%%%%%%%%%%%%%%%% week18 day2
  
  ****vault 
ansible-vault encrypt
ansible-vault decrypt
ansible-vault edit 
ansible-vault rekey

  
Testing
-yamllint
--syntax-check
ansible-lint
molecule ( test env)
ansible-playbook --check
parallel infrastructure


%%%%%%%%%%%%%%%%%%%%%%% windows as ansible target machine
 
** Check if openssh is installed

 Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*' 
** install openssh server

Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0

** check sshd daemon
Get-Service sshd

** start sshd daemon

 Start-Service sshd
** enable daemon
Set-Service -Name sshd -StartupType 'Automatic'


*** inventory file
[db]
172.31.42.154
[ws]
172.31.20.62

[wind]
3.138.200.43

[ws:vars]
ansible_ssh_private_key_file=~/ansible-key.pem
ansible_user= ubuntu

[db:vars]
ansible_ssh_private_key_file=~/ansible-key.pem
ansible_user= ec2-user

[wind:vars]
ansible_user=Administrator
ansible_password=$wqCIR(a;4SN%Iv33bvUep*hG-$8Tp3o
ansible_connection=ssh
ansible_shell_type=cmd
                        
** to solve the ssh pass error
                        
sudo amazon-linux-extras install epel -y
udo yum install sshpass -y                        

**** windows.yaml
                        
---
- hosts: wind
  tasks:
    - name: Create a directory
      win_file:
        path: C:\Users\Administrator\Desktop\serge
        state: directory
    - name: download file online 
      win_get_url:
        dest: C:\Users\Administrator\Desktop\serge
        url: https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.9.3.tar.xz

    - name: Update all package
      win_updates:
        category_names: '*'
        reboot: true
                    
                        
 **** jenkins patching job
                        
 pipeline{
    agent {label 'ansible'}
    
    stages{
       stage('run playbook'){
            steps{
                dir('/home/ec2-user/ansible-dev'){
           // sh 'pwd'
            sh 'ansible-playbook play1.yml'
        }
            }
       }
    }
}                       
    *** question 6 playbook
                        
---
- hosts: ws
  become: yes
  become_user: root 
  ignore_errors: yes
  tasks:
    - name: Delete Old Artifact
      file:
        path: /opt/qa/spg/*.jar
        state: absent
    
    - name: Get the PID
      shell:  lsof -t -i :8082
      register: pid_output
    
    - name: Kill Running App
      shell: "kill -9 {{ pid_output.stdout }}"
      when: pid_output.stdout != ''

    - name: Download New Artifact  
      get_url:
        url: http://34.201.161.49:8081/artifactory/geolocation/geo/bio_17.jar
        dest: /opt/qa/spg
        url_username: admin
        url_password: password

    - name: Run the App
      shell: nohup java -jar /opt/qa/spg/*.jar > /opt/qa/spg.log 2>&1 &
             
   %%%%%%%%%%%%%%%%%%%%%% week19 K8s
                        
1- what is Kubernetes and why Kubernetes ?
	- open source container orchestration tools. ( moving from monolithic app to microservices)

  - def: open source tool develop in GO by google to manage containerized applications(orchestration).
   k8s ==> k to s we have 8 characters.
   kubernetes cluster ==> Master or control plane + nodes or worker nodes
  
                        
 2- Architecture and Components:
  - Master: 
     - api server 
     - scheduler
     - controller manager
     - etcd
     - cloud controller manager ( for cluster running in the cloud )
  - Node:
    -kubelet (contains the cAdvisor that collects statistics about resources usage for containers running on a node)
    -container runtime ( docker , cri-o, containerd )
    -kube-proxy
*NB: master also has kubelet, kube-proxy , container runtime because it runs some system pods 


3- kubernetes objects:
	- Basic objects: Pod, Service, volumes, namespaces, configmaps, secrets...
  - High level objects: Replicaset, deployment, statefulset, Daemonsets, ingress, jobs...

4- Kubernetes features:
  - High availability ( no downtime )
  - Auto Scalability ( scale up or down depending on the load  manually and automatically )
  - Disaster recovery ( backup )
  - Self healing
  - Secret and configuration management ( secrets and configmaps )
  - Load balancing and service discovery
  - Automated rollouts and rollbacks
  - production Support
                         
 5- Managing cluster 
  - UI  ==> openlens
  - API
  - cli ==> kubectl
            kube cuddle
            kube cutle

6- Cluster and kubectl cli setup:
  - Installation:
    non prod: minikube, docker desktop, kind, 
    prod: kubeadm, kops, kubespray, 
    cloud: AKS, EKS, GKE, LKE, RKE
	                 
terraform: https://dataservicegroup.atlassian.net/l/cp/kZJWNt9x
Bare metal on Linode ubuntu 22.04 servers: https://dataservicegroup.atlassian.net/l/cp/SC0rZvxe
                        

** color terminal
                        
echo "PS1='\e[1;32m\u@\h \w$ \e[m'" >> /root/.bash_profile
su - root
cd app-config                        
                        
7- *** Practice using the cluster: Link to the github repository: https://github.com/utrains/kubernetes-practice1 

kubectl get nodes
kubectl describe nodes nodename
kubectl get pods 
kubectl get pods -A 
kubectl get pods -A -o wide 
kubectl get deployment
kubectl get service 
kubectl get namespace

*** deploy a pod 

kubectl run app1 --image nginx
kubectl get pod
kubectl get pod -o wide 
kubectl run app2 --image nginxjdf
kubectl get pods
kubectl get pod -o wide
kubectl describe pod app2
kubectl edit pod app2
kubectl get pods
kubectl logs app1 or app2
kubectl delete pod podname

*** deploy pod with config file ( yaml, json )

kubectl run app1 --image nginx --dry-run=client -o json
kubectl run app1 --image nginx --dry-run=client -o yaml > pod.yaml
kubectl create namespace devops --dry-run=client -o yaml > namespace.yaml 
                        
*** pod.yaml
                        
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: app2
  name: app2
spec:
  containers:
  - image: redis 
    name: app2                        
 
 *** second pod.yaml
                        
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: app2
  name: app2
  namespace: devops
spec:
  containers:
  - image: redis
    name: app2  
                        
*** deploy.yaml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest
                        
**** Deployment     
kubectl apply -f deploy.yaml
kubectl get deployment
kubectl describe deploy nginx-deployment     
kubectl get rs
kubectl get pods
kubectl get all 
kubectl describe deployment <deployment name>
kubectl edit deployment  #( change on the fly )
# display yaml output of a deployment with status 
kubectl get deployment nginx-deployment -o yaml > deployment-result1.yaml
kubectl apply -f deploy.yaml #( change rs , change image , change label )
# check all the revisions of the deployment
kubectl rollout status deployment/nginx-deployment
kubectl rollout history deployment/nginx-deployment
# if you decide to rollback to a previous revision,
kubectl rollout undo deployment/nginx-deployment --to-revision=2 
                        
 **** Service ( use to expose deployment.) 
type of services:

- ClusterIP  ( internal access only. so can only be access within the cluster )
     
apiVersion: v1
kind: Service
metadata:
  name: utrains-service                           
spec:
  selector:
    name: utrains
  ports:
  - name: utrains-service-port
    protocol: TCP
    port: 80
    targetPort: 80     
  
- NodePort: expose app from a node ip thru a port ( 30,000 to 32,767 )
     
apiVersion: v1
kind: Service
metadata:
  name: utrains-service
spec:
  type: NodePort
  selector:
    name: utrains
  ports:
    - port: 80
      targetPort: 80
      # optional
      nodePort: 30007 
     
- LoadBalancer:  ( this will expose the app externally to the world. in the cloud case the cloud controller manager will create an elb)

apiVersion: v1
kind: Service
metadata:
  name: utrains-service
spec:
  type: LoadBalancer
  selector:
    name: utrains
  ports:
    - port: 80
      targetPort: 80

kubectl get svc 
kubectl describe service <service-name>
kubectl delete -f <servicefile>
- externalName: ( use to point to external name like database link or another third party tools with url)
     
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com   
                        
 ********-QoS 
2- Quality of service (QoS)
   - Guaranteed (the request and limit defined in the resource specification must the same)
     Last to be evicted
 ##################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: qos-guaranteed
  namespace: qos-example
  labels:
   app: utrains
spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "250Mi"
          cpu: "400m"
        requests:
          memory: "250Mi"
          cpu: "400m"
##################################################################################
  
   - Burstable (the limits of the CPU or Memory defined in the specification must higher than the requests)

##################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: qos-burstable
#  namespace: qos-example
  labels:
   app: utrains
spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "250Mi"
        requests:
          memory: "150Mi"
##################################################################################
    
   - BestEffort (no request or limit defined)
    
##################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: qos-besteffort
  namespace: qos-example
  labels:
   app: utrains
spec:
  containers:
    - name: nginx
      image: nginx 
                        
 %%%%%%%%%%%%%%%% week 20
                        
                        
 *******Advanced scheduling.
     nodeName:
     node selector:
     node affinity/node anti affinity:
     pod affinity/anti affinity:
     Taints and toleration:
 
**a- NodeName
##################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains2
 # namespace: dev
  labels:
   app: utrains2
spec:
  nodeName: nodeone.example.com
  # Replace nodeone.example.com with your own node name
  containers:
    - name: utrains-app
      image: httpd
##################################################################################

kubectl create -f pod-with-node-name.yaml
kubectl get pods 
# make sure the node was placed on the node having the specified name
kubectl get pods -o wide
kubectl delete pod utrains2 
     
    ** b- Node selector 
			Here you label the node Then use node selector in the pod specification using the nodeSelector field
# label a node
kubectl get nodes
# kubectl label nodes node-name-here key=value
kubectl label nodes node01 sku=small
kubectl get nodes --show-labels | grep sku

#create a pod with a specific node selector that matches the label:

###################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains
#  namespace: dev
  labels:
   app: utrains
spec:
  nodeSelector:
        sku: small
        en: dev
  containers:
    - name: utrains-app
      image: httpd
  
        
##################################################################################
kubectl create -f pod-with-node-selector.yaml
kubectl get pods 
# check that the pod was placed on the labeled node
kubectl get pods  -o wide
kubectl delete pod utrains             
   
    
***
          
    b- Node affinity
        Here we want to schedule a pod on a specific node which has some features 
      (example a type of memory, CPU etc). There are two variances:
     
NB: We have the following operators (In , NotIn , Exists , DoesNotExist , Gt(greater than) , Lt (Lesser than))
               
        - requiredDuringSchedulingIgnoredDuringExecution (Hard type):
        - preferredDuringSchedulingIgnoredDuringExecution ( soft type)
          
##################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains
 # namespace: dev
  labels:
   app: utrains
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: sku
            operator: In
            values:
            - small       
  containers:
    - name: utrains-app
      image: httpd

        
##################################################################################
# label nodes
kubectl get nodes
kubectl label nodes node01 sku=medium
kubectl label nodes node02 sku=small
kubectl get nodes --show-labels | grep sku
kubectl get nodes -l sku=medium
# create and check where the pod is placed
kubectl create -f pod-with-required-node-affinity.yaml
kubectl get pods -o wide
kubectl delete pod utrains 

- preferredDuringSchedulingIgnoredDuringExecution (soft type):
    
    
##################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains3
#  namespace: geolocation
  labels:
   app: utrains3
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: sku
            operator: In
            values:
            - medium
  containers:
    - name: utrains-app
      image: httpd
        
  
  affinity:
   nodeAffinity:
     preferredDuringSchedulingIgnoredDuringExecution:
     - preference:
         matchExpressions:
         - key: example.com/myLabel
           operator: In
           values:
           - a
       weight: 40
     - preference:
         matchExpressions:
         - key: example.com/myLabel
           operator: In
           values:
           - b
       weight: 35
##################################################################################
# label nodes
kubectl get nodes --show-labels | grep sku
# create and check where the pod is placed
kubectl create -f pod-with-preferred-node-affinity.yaml
kubectl get pods  -o wide
kubectl delete pod utrains3 

		c- Node anti-affinity
 We use the NotIn operator to set the anti affinity. 
We have the following operators (In , NotIn , Exists , DoesNotExist , 
                          Gt(greater than) , Lt (Lesser than))
####################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains1
 # namespace: dev
  labels:
   app: utrains1
spec:
  containers:
    - name: utrains-app
      image: httpd
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: sku
            operator: NotIn
            values:
            - small
####################################################################################
kubectl create -f pod-with-node-anti-affinity.yaml
kubectl get pods -o wide
kubectl describe pod utrains1 
kubectl delete pod utrains1 
        
    d- Pod affinity/anti-affinity
    
       Here, we either want to schedule pod A near pod B 
      or to make sure that pod A never gets schedule on a node where pod B has already been scheduled
      
# pod A just a sample pod
################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: web
#  namespace: dev
  labels:
   app: web
spec:
  containers:
    - name: utrains-app
      image: httpd
################################################################################
kubectl create -f podA.yaml
kubectl get pods -o wide

# pod B to be placed near podA: Pod Affinity

################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains
#  namespace: dev
  labels:
   app: utrains
spec:
  containers:
    - name: utrains-app
      image: httpd
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web
        topologyKey: "kubernetes.io/hostname"
################################################################################
kubectl create -f podB.yaml
# make sure pod B is placed near podA
kubectl get pods -n dev -o wide

# pod C not to be placed near podA: Pod Anti affinity

################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: utrains1
#  namespace: dev
  labels:
   app: utrains1
spec:
  containers:
    - name: utrains-app
      image: httpd
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web
        topologyKey: "kubernetes.io/hostname"
################################################################################
kubectl create -f podC.yaml
# make sure pod C is not placed near podA
kubectl get pods  -o wide

# delete the pods and remove the labels on nodes
kubectl delete web 
kubectl delete utrains 
kubectl delete utrains1 
kubectl label node node01 sku-
kubectl label node node02 sku-
     
*********************- Taints and tolerations
        
       Here, we also want to make sure that pods do not get scheduled on inappropriate nodes. 
       The taint set on a node will attract only nodes that have the corresponding tolerations and will repel others
      
# Taint the nodes
# kubectl taint nodes node-name key=value:taint-effect  
**taint-effects: 
     NoSchedule, ==> do not schedule new pod
     PreferNoSchedule, ==> schedule only if no other choices
     NoExecute, if a pod was already running that pod needs to exit
     
kubectl taint nodes node01 color=pink:NoSchedule
kubectl taint nodes node02 color=yellow:NoSchedule
  
# Set the toleration on the pod: key/value/effect 
**operator option: Equal or Exists
# pod1 with operator Equal
####################################################################################
apiVersion: v1
kind: Pod
metadata:
  name: webserver1
#  namespace: dev
spec:
  containers:
    - name: webserver1
      image: httpd
      ports:
        - containerPort: 80
          name: http
          protocol: TCP
  tolerations:
  - key: "color"
    operator: "Equal"  # 
    value: "pink"
    effect: "NoSchedule"    
                        
*** Practice on kubernetes pods, services, replicaset, deployment
https://dataservicegroup.atlassian.net/l/cp/F4t3PnfY 
***** Configmaps
   to keep your application code separate from your environment configuration
   used for non confidential data
# create configmap in the command line (can be created form literal values, files, directories)
   kubectl create configmap configmap-name --from-literal key=value
   kubectl create configmap configmap-name --from-file path/to/file
# Create configmap from yaml file
###################################################################
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  mydata: hello-world
###################################################################

# use configmap in the pod
###################################################################
apiVersion: v1 
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    env:
      - name: cm
        valueFrom:
          configMapKeyRef: 
            name: my-configmap 
            key: mydata
              
              
              
###################################################################
***Practice on configmaps: https://dataservicegroup.atlassian.net/l/cp/gRz1F5kd
  
H- Secrets
   just like Configmap but it is used to store sensitive data like private credentials, password, tokens, keys
   Here we want to store a username devops and a password 1f2d1e2e67df in a secret object. First, we need to  convert the values to base64
    
echo -n 'devops' | base64
echo -n '1f2d1e2e67df' | base64

Secret definition
####################################################################
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: ZGV2b3Bz
  password: MWYyZDFlMmU2N2Rm
####################################################################

Pod for secret definition:

####################################################################
apiVersion: v1
kind: Pod
metadata:
  name: my-secret-pod1
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
      - containerPort: 80
      env:
        - name: SECRET_USERNAME
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
        - name: SECRET_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: password
####################################################################
apply the pod then login to the pod to see the secret information are accessible
kubectl exec -it my-secret-pod1 -- printenv

*** Practice on Secrets: https://dataservicegroup.atlassian.net/l/cp/wwJkT016
  
E- Daemonsets
   object used in Kubernetes to ensure that a copy of a desired pod is run on all (or some) nodes of the cluster.
   It can be used for logs collection(Fluentd, logstash), cluster storage, log rotation and cleaning logs, node monitoring
  
#################################################################
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nn
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2                       
                        
 kubectl get daemonset
                        
 %%%%% deploy microservice app in cluster %%%%%%%%%%%%%
                        
 1- wordpress app
 
                        
 *** mysql.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  mysql
  namespace: default
  labels:
    app:  mysql
spec:
  selector:
    matchLabels:
      app: mysql
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app:  mysql
    spec:
    
      containers:
      - name:  mysql
        image:  mysql:5.6
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: mysql_pass
        ports:
        - containerPort:  3306
          name:  mysql
                        
 *** secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
  namespace: default
type: Opaque
data:
  mysql_pass: bWFzdGVyCg==                       
                        
 ***** mysql-svc.yaml
                        
apiVersion: v1
kind: Service
metadata:
  name: mysql-svc
  namespace: default
spec:
  selector:
    app: mysql
  type: ClusterIP
  ports:
  - name: mysql-svc
    protocol: TCP
    port: 3306
    targetPort: 3306
                            
 *** wordpres.yaml
                        
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  wordpress
  namespace: default
  labels:
    app:  wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
  replicas: 3
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app:  wordpress
    spec:
      containers:
      - name:  wordpress
        image:  wordpress:4.8-apache
        env:
        - name: WORDPRESS_DB_HOST
          valueFrom:
            configMapKeyRef:
              name: WCM
              key: data_host
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: mysql_pass
        ports:
        - containerPort:  80
          name:  wordpress             

                        
**** configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: WCM
data:
  data_host: mysql-svc                        
                        
**** wordpress-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  namespace: default
spec:
  selector:
    app: wordpress
  type: LoadBalancer
  ports:
  - name: wordpress
    protocol: TCP
    port: 80
    targetPort: 80
                            
                        
                        
 2- mongodb and express app                       

mongo-app:                        
mongo image: mongo:4.4.3
MONGO_INITDB_ROOT_USERNAME: root
MONGO_INITDB_ROOT_PASSWORD: PassW09@!
MONGO_INITDB_DATABASE: admin                        
Ports: 27017
                        
mongo-express    ==> external or entrypoint                    
image: mongo-express:0.54.0
ME_CONFIG_MONGODB_ADMINUSERNAME: same as mon username
ME_CONFIG_MONGODB_ADMINPASSWORD: same as mongo password                        
ME_CONFIG_MONGODB_SERVER: is the mongo service                        
port 8081                        

                        
****** sol:
** secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongo-secret
  namespace: default
type: Opaque
data:
  mongo_pass: UGFzc1cwOUAh
  mongo_username: cm9vdA==
                          
*** mongo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  namespace: default
  labels:
    app: mongo
spec:
  selector:
    matchLabels:
      app: mongo
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mongo 
    spec:
    
      containers:
      - name: mongo 
        image: mongo:4.4.3
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: mongo_username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: mongo_pass
        - name: MONGO_INITDB_DATABASE
          valueFrom:
            configMapKeyRef:
              name: mongo-cm
              key: mongo_initial_db
        ports:
        - containerPort: 27017
          name: mongo 
                        
                        
**** express.yaml
                        
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
  namespace: default
  labels:
    app: mongo-express
spec:
  selector:
    matchLabels:
      app: mongo-express
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mongo-express  
    spec:
    
      containers:
      - name: mongo-express 
        image: mongo-express:0.54.0
        env:
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: mongo_username
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: mongo_pass
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom:
            configMapKeyRef:
              name: mongo-cm
              key: data_host
        ports:
        - containerPort: 8081
          name: mongo-express 
---
apiVersion: v1
kind: Service
metadata:
  name: mongo-express
  namespace: default
spec:
  selector:
    app: mongo-express 
  type: LoadBalancer
  ports:
  - name: mongo-svc
    protocol: TCP
    port: 80
    targetPort: 8081                                       
                        
**** mongosvc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo-svc
  namespace: default
spec:
  selector:
    app: mongo 
  type: ClusterIP
  ports:
  - name: mongo-svc
    protocol: TCP
    port: 27017
    targetPort: 27017   
                        
 *** configmap.yaml
                        
kind: ConfigMap
apiVersion: v1
metadata:
  name: mongo-cm
data:
  data_host: mongo-svc
  mongo_initial_db: admin                        
                        
                        
****** K8s setup using Kind  ==> deployment using terraform 
                        
mkdir kind
cd kind
git clone https://github.com/utrains/kind-k8s-terraform.git   
cd kind-k8s-terraform
terraform init
terraform validate
terraform plan
terraform apply --auto-approve
use the ssh command on the output to access the server, 
then you can execute all your kubectl commands.                        
                        
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% week 21 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                        
 %%%%%%%%%%%%%%%%%% Monitoring in entreprise%%%%%%%%%%%%%%
     
     
*** Splunk 
  http://50.116.20.67:8000
username: admin
password: abcd1234

project:

At work we have some issues happening with some servers and you need to investigate. the best place 
to start will be to check the logs from the monitoring tool: please go ahead, log in to the work splunk ui
and see if you can find something.     

NB: every action on the server is thru sudo and the logs are stored in /var/log/auth.log     
     
1- The server jfrog went down on the 12th of July between 8h40pm and 8h50pm, please investigate and see if you can find out what the issue was.
(This info can come from a user or an alert from the monitoring tool/team)                        
2- The artifactory application running on the jfrog server went down on the 13th of July  at 1:02:00pm and you need to figure out why and if possible who was working on it.     
3- try to get the list of all users that have logged in to jfrog host on 8th of July.
4- check all the user that have used sudo command in the last 48 h on host jfrog
6- check if a user with username Serge have accessed the server in the last 24h.
7- Check all the 500 error from apache app on the host jfrog.utrains.com ; possibly the date it happen                     

                        
*****Practice on your personal lab:
** Lab setup 
      https://dataservicegroup.atlassian.net/wiki/spaces/DT/pages/2210627585/Deploying+Splunk+Enterprise+and+Splunk+Forwarder
1- On the splunk server, configure a second forwarding and receiving on 8080 port
2- check that you are getting data Forwarded  in realtime in your splunk server.
3- a server without hostname is showing in splunk with the ip address how to assign specific hostname to the splunk forwarder?

cd /opt/splunkforwarder/bin/
sudo ./splunk set default-hostname <new-hostname>
sudo ./splunk restart
     
4- install apache on the forwarder make sure port 80 is opened and add /etc/httpd/logs as a new path to monitor

sudo yum install httpd -y
sudo service httpd start
sudo chkconfig httpd on
curl localhost
sudo ./splunk add monitor /etc/httpd/logs
sudo ./splunk restart
while true; do curl localhost; curl localhost/sakhlasfdn/; sleep 2; done ## this will generate logs
     
5- Go into the splunk server and see if you can view the access and error log 
6- hit the browser for your apache server couple time and see if you getting data in real time in your splunk server via the access log
7- we have log files that needs to be analyzed in splunk ( access log in the www1 folder) 
the link to the file is https://github.com/utrains/splunk/raw/main/tutorialdata.zip
downlaod the zip file  , unzip it and  upload it into splunk. so we can start indexing the data.
NB: this confluence page shows how to upload data into splunk:
    https://dataservicegroup.atlassian.net/l/cp/DNcXPNyt

8- follow the document in 7- to practice searching thru data uploaded. 
9- on the forwarder, we need to record all the users using sudo. configure a file on the system to
record all the sudo log. called sudo.log and put it in /var/log
Test: run some sudo commands on the forwarder and go check in splunk UI if you see the logs.
steps to setup sudo command logs:
     visudo  or vi /etc/sudoers  and add below line then save the file
     Defaults logfile="/var/log/sudo.log"                        
                                                

%%%%%%%%%%%%%%%%%%%% week 22 Python Scripting

                        
****Note: Python scripting is important for Sys admin, DevOps and cloud engineers because it helps automate repetitive tasks, 
manage and set up cloud resources, and create custom monitoring tools. It makes managing systems easier and more 
efficient by handling things like configuration and data analysis.    

why python instead bash shell script ? : 
  
Python is often preferred over Bash for scripting because it offers 
- better readability, 
- extensive libraries, 
- cross-platform compatibility,
- robust error handling, 
- support for advanced features and more  
    
1- Intro to python:
  
  - What is Python
  	* Python is a high-level, interpreted and object-oriented programming language.
    created by Guido van Rossum 1991
    * It is flexible, easy to learn and has a very large community.
  - what is it used for?
  
    * Automate DevOps Tasks
    * API Development
    * Selenium Testing with Python
		* Web development (django or flask)
    * Data Science and Machine learning (Recommendations systems, face recognition system)
    * Web scraping: collect and analyze data from websites
    
  - Use case of  python script in  DevOps realm:
  	- You can use GITAPI, a pure python API that interacts with version control system
    - Use OS and SUBPROCESS python modules to handle environment specifics, networking and OS specifics
    - BOTO3 is a python SDK module for AWS resources. It enables Python developers to create, configure and manage AWS services
    - Testing frameworks like Selenium can be used for automation testing using Python
    - Python has modules to manage almost all databases
    - Python has modules like fabric, fabtools and cuisine used to manage deployments 
  
  *	System Admin tasks
  *	Server/Os Admins
	*	WebServer Admins
	*	MiddleWare Admins
	*	Db Admins
	*	Storage Admins
	*	Cloud Admins, etc..

	*** Why Python Specifically
      * Easy to learn
			*	Platform Independent
	 		*	Python is also very famous for its simple programming syntax, code 
			*	Readability and English-like commands that make coding in Python lot easier and efficient.
			*	Lot of Pre-defined and third party modules.
			*	Python take very less time to develop, as Python codes are 3 to 5 
				times shorter than Java codes.

  - which companies used it
  	* Facebook, Google, Instagram, Pinterest, Amazon, Dropbox, Uber, Reddit, Spotify and Netflix
    
  - Python2 vs Python3  ( 3.x is not an update ) development of 2.x has stopped since April 2020
  	* Print keyword:
    	for python 2.x : print is a statement not a function. Example: print "Helloworld"
      for python 3.x : print is a function not a statement. Example: print("Helloworld")
    * Iteration:
      for python 2.x: xrange() function is used for iterations
      for python 3.x: range() function is used for iterations
        
    * Integer division:
      for python 2.x: When two integers are divided, the result is an integer value
      for python 3.x: When two integers are divided, the result is a float value
        
  - Install python
    Go to the official download page of python through this link, https://www.python.org/downloads/
    Click on the link related to your operating system
    Choose Python 3.12.4 or latest version and download it according to your operating system
    Click on the link to download Windows x86-64 executable installer
    
  	* for Windows:
      When the download is complete:
        Open the folder containing the downloaded file
        Double click on the downloaded setup package to launch the installation
        Follow the on-screen installation instructions (we recommend to leave the default settings)
        Make sure you click on add Python 3.x to PATH check box to include the interpreter in the execution path
        Click on install Now, then wait till the installation complete.
        
		* for Mac OS:
      After downloading the python installer for MAC OS:
        Open the folder containing the downloaded file
        Double click on the downloaded setup package to launch the installation
        Follow the on-screen installation instructions (we recommend to leave the default settings)
        Lastly, you will have Python running on your Mac OS system.
    

		* for Linux:
      
      * On Centos 7, redhat 7, amazon linux 2 :
        
        	$ sudo yum update -y
          $ sudo yum install python3 -y
          
      * with package:    
      After downloading the python installer for Linux:
        Open the folder containing the downloaded file (.tar.xz)
        Extract the tarball: $ tar -xf Python-3.?.?.tar.xz
        Once the Python tarball has been extracted, navigate to the configure script and execute it in your Linux terminal with:
          $ cd Python-3.*
						./configure
      	Start the build process:
          * If you already have a version of Python installed on your system and you want to install the new version alongside it, use this command:
            $ sudo make altinstall
          * If you want to replace your current version of Python with this new version, you should uninstall your current Python package using your
          package manager (such as apt or dnf) and then install:
            $ sudo make install
  
  - Verify the installation of python:
    $ python3 --version
    $ python --version
  
   **** login to a linux box (any ) on different terminal:
                        
   ssh u5bt@50.116.20.67
   password: abc                     
                        
                        
  ** Operators ( part 1 ) 
  
  - Arithmetic operators are used with numeric values to perform common mathematical operations
    		
      * Addition (+) 
      >> 5+7
      >> 17.4 + 7
      >> 12.5 + 8.3
      * Substraction (-)
      >> 15-7
      >> 17.4 - 7
      >> 12.5 - 8.3
      * Multiplication (*)
      >> 5*7
      >> 17.4 * 7
      >> 12.5 * 8.3
      * Division (/)
   		>> 5/7
      >> 17.4 / 7
      >> 12.5 / 8.3
      * exponential
      a**b  ==> a at the power of b
      
      *floor division
      a//b round to result to the lower value
      
      *Modulus
      a%b give the remender in this division
      
  - Create and run your first program:
    
    * Create a file helloworld.py with the following content:
      print("Hello world !!!")
  	* Run your first program:
      python3 helloworld.py or python helloworld.py
  - single or double or triple quote:
    print('it's a sunny day')  ==> print("it's a sunny day") or print('it\'s a sunny day')
    triple quote will help print multi line in the exact format we want like emails:
     print("""
          Hello Serge,
      we have received your payment.
      Thanks and Regards
      The Utrain Team
     """)     
          
  - Comments in python
  	Comments are used to explain python code or make it more readable.
    
    A comment in python starts with "#", and python will ignore what is writing behind "#" in the same line.
    If you want the interpreter to ignore multiple lines, put the lines between triple single or double quotes (""" """).
  	
    Example: 		# this is a python comment in one line
      					print("comment in one line")
        				
             ** comment multiple lines:
          			"""
                
                this is a python
                comment written in
                multiples lines
                
                """
            		print("multiline comment")
          
	- Variables
  
    A variable is a fundamental concept in any programming language.
    It is a reserved memory location that stores and manipulate data.
    
    ** Assignment operators are used to assign values to variables

      Assignment (=),  ==> a = b 
      Assignment the sum (+=),  a+=b ==> a = a+b
      Assignment the difference (-=),	==> a-=b ==> a = a-b
      Assignment the quotient (/=), ==>  a/=b ==> a = a/b
      Assignment the product (*=)  ==>  a*=b ==> a = a*b
    	
    * Variables names
    	To create a variable, we use the syntax below:
				variable_name= variable_value
        
			Rules for creating variables in python:
      A variable name must start with a letter or the underscore character.  Example: name or _name 
      A variable name cannot start with a number
      A variable name can only contain alphanumeric characters and underscores (A-z, 0-9, and _ )
      Variable names are case-sensitive (name, Name and NAME are three different variables)
      The reserved words(keywords) cannot be used naming the variable. Example of keywords: if,import, pass, try, from, for, except, def, else, while … etc
			
      Examples of legal variables:
          
        myname = "Serge"
        my_name = "Serge"
        _my_name = "Serge"
        myName = "Serge"
        MYNAME = "Serge"
        myname2 = "Serge"
        
      Examples of illlegal variables:
          
        4myname = "Serge"
        my-name = "Serge"
        my name = "Serge"
    
    * Assign unique or multiples values to variables
    	A unique value can be assigned to multiples variables in python.
      	Example: var1 = var2 = var3 = "python"
    
    	Many values can also be assigned to many variables in one line. In this case, the value of a variable is the one at the same position.
      	Example: var1, var2, var3= "linux", "devops","python"
   
          
          
    - Users inputs ( input function)
  
    In python, we ask a user for input the input() method.
    The syntax is: 
      input(prompt), where prompt is the string we wish to display on the screen. It is optional.
        Practice: 	let’s ask for user age and display it.
                  age= input("What's your age ?") 
                  print(age)
                  print(type(age))
   Note: every data captured from input is by default a string.
          
  ** Data Type ( Part 1 ):
    
          
    - Numbers (Integer, float, complex) 

    
    	Numbers are used in python to store numeric values. There are three types of numbers in python: Integer, Float and Complex.
        
      	Integer number is represented by int class. It contains positive or negative whole numbers (without fraction or decimal).
        	Example: age= 26, counter= -12 
        
				Float number is represented by float class. It is a real number with floating point representation. It is specified by a decimal point.
      		Example: weight= 67.6
            
      	Complex numbers are written with a "j" as the imaginary part.
         	Example: b= 4+3j 
#Number 
a= 20  # int
b= 2.4  # float
c= 3+7j  # complex
          
    * type function: used to check the data type.
    type(variable)
          
    - Booleans 
    	
        Boolean is the simplest data type of all and has just two values assigned to it: True or False.
          Practice: 	boolean1= 7 > 7.5		=> False
                    	boolean2= 7 == 7.5	=> False
                    	boolean3= 7 < 7.5		=> True
          
        is_New_Student = True 
          
          
    - Strings

      A string can be defined as the sequence of characters represented in the quotation marks.
        Example: 'welcome', "welcome" , """helo there python"""
      Assign a String to a variable.
        Example: greeting= "Hello"
      It is also possible to assign a multiline string to a variable by using three quotes or three single quotes.
        Example: greetings1='''welcome at Utrains;
                              we are glad to be together during this training'''
                greetings2="""welcome at Utrains;
                              we are glad to be together during this training"""
    *** String operations:
        * len() function:
          used to count the number of characters in a string including space.
          example: len('serge') will be 5
                   len("it is a good day") will be 16
          
        *Slicing strings in python/ indexing :
          in a string , each character represent an index. ( the first is index 0 and so on, the last is index -1 ...)
          
          Example: greeting="Hello everyone!!"

          Normal slicing:	print(greeting[2:8]) => "llo ev"
          Slice from the start:	print(greeting[:10]) => "Hello ever"
          Slice to the end:	print(greeting[5:]) => " everyone!!"
          Slicing on negative indexes:	print(greeting[-6:-1]) => "yone!"

        Concatenate strings: To concatenate or combine two strings, you can use the + operator.
          Example: firstname="estephe"
                    surname="Kana"
                    fullname1= firstname + surname => "estepheKana"
                    fullname3 = firstname + " " + surname
              
    *	Strings Methods: ( String methods simplify and speed up text manipulation tasks, 
                        making processing and formatting easier and more efficient.)
      
        Upper Case: The upper() method returns the string in upper case.
          Example:  x= "Hello, World!"
                    print(x.upper()) => "HELLO, WORLD!"
              
				Lower case: The lower() method returns the string in lower case.
          Example: 	x= "Hello, World!"
            				print(y.lower()) => "hello, world!"
              
        Replace String: The replace() method replaces a string with another string.
          Example: 	x= "Hello, World!"
            				print(x.replace("Hello", "Good morning")) => "Good morning, World!"
              
        islower: The islower() method checks if all the characters in the string are in lower case
          Example: 	
            #will return true because all characters in string '5a' is in lower case.
						print('5a'.islower())
            #will return true,all characters in string 'practice python coding ' is in lowercase.
            print('practice python coding'.islower())
            #will return false,all characters in string 'Abc' is not in lower case.
            print('Abc'.islower())
            
        isupper: The isupper() method checks if all the characters in the string are in upper case
          Example:
            #will return false because all characters in string '5a' is not in upper case.
            print('5a'.isupper())
            #will return false,all characters in string 'practice python coding ' is not in uppercase.
            print('practice python coding'.isupper())
            #will return True ,all characters in string 'ABC' is  in upper case.
            print('ABC'.isupper())
            
      	split: The string method .split() splits a string into a list of items.
          Example:
            #when you don't provide any argument,whitespace will be used as a seperator
            var1="Be the change that you wish to see in the world"
            a_list=var1.split()
            print(a_list)
            
            #passed 'wish' seperator as an argument
            var1="Be the change that you wish to see in the world"
            a_list=var1.split('wish')
            print(a_list)
        
        find: The Python string method .find() returns the index of the first occurrence of the string passed as the argument. 
          		It returns -1 if no occurrence is found.
          Example:
            #the first instance of search string 'my' is found at index 7,8
            #So,it will return 7
            a_string='I love my country,my country is great'
            print(a_string.find('my'))
            
            #find() also accepts start and end index as an argument.
            #it will search the string from index 8 to 25
            #and will return the lowest index of 'my' if found
            print(a_string.find('my',8,25))
            #find() will return -1 if searched string is not found in given string.
            print(a_string.find('python'))
        
        isdigit: The isdigit() method checks if all the characters in the text are digits
          Example:
            #will return false because all elements in string 'abc' are alphabets not digits
            print('abc'.isdigit())

            #false because there is space
            #all characters should be digit,for returning true
            name='serge Kamgang B'
            print(name.isdigit())

            #true because all characters are digits
            print('123'.isdigit())
            
		- Conversion between Data types (Casting)
  		
    	We can convert different data types by using different types of casting functions like int(), float(), str(), etc.
      To convert an integer to a float, we use float() function.
      	Example: 	float(9)	=> 9.0
      To convert from float to int will truncate the value (make it closer to zero).
      	Example: 	int(67.6)		=> 67
          				int(-67.6)	=> -67	
      To convert from string to float, we use float() function. string must contain compatible values.
      	Example: 	float('67.6')	=> 67.6
    	To convert from an integer to a string, we use str() function.
      	Example: 	str(54)	=> '54'
      Conversion of one sequence (tuple, set, list) to another.
				Example: 	set([1,2,3]) 		=> {1,2,3}
          				tuple({4,5,6})	=> (4,5,6)
            			list('Hello')		=> ['H', 'e', 'l', 'l', 'o']
      To convert to dictionary, each element must be a pair.
				Example: 	dict([[7,8],[9,10]])		=> {7:8, 9:10}
          				dict([(2,21), (4,47)])	=> {2:21, 4:47}
	
 Example with input function:
          Whatever you enter as input, the input function converts it into a string. 
    So, to take the input in the form of int you need to use int() method along with the input function.
      Practice: 	let’s ask a number to a user, convert it into int,  then add it to 10 and print the answer.
                  num=int(input('Enter a number: '))
                  sum= num+10
                  print(type(num))
                  print(sum)

    Taking input as a float: When the user enter an integer or float it’s work well. But when he enter a string, a ValueError occurred.
      Practice: 	num=float(input('Enter a number: '))
                  sum= num+10
                  print(type(num))
                  print(sum)
  -	Formatting

    The format() method allows you in python to combine strings and numbers
    The format() method takes the passed arguments, formats them, and places them in the string where the placeholders {} are.
      Example:	number_oranges= 10
                bag="The exact number of oranges inside my bag is {}"
                print(bag.format(number_oranges)) => "The exact number of oranges inside my bag is 10

    Instead of using format() method you can use the keyword f as follows:
      Example:	number_oranges= 10
                print(f"The exact number of oranges inside my bag is {number_oranges}") => "The exact number of oranges inside my bag is 10                   
          
  - Operators ( part 2 )
        
      *	Comparison operators are used to compare two values
          Greater than (>); less than (<); Greater than or equal to (>=); less than or equal to (<=); Equal to (==); Not equal to (!=)
       f_num=5
       s_num=10 
          
      *	Logical operators are used to combine conditional statements: and, or, not
          

      *	Membership operators are used to test if a sequence is present in an object: in , not in
       example:
          
      'a' in 'serge'  ==> bool
      '@' not in 'kserge@gmail.com'  ==> bool
    
  *** check python docs for more operators.
          
  - Control flow (Part 1) 
  
    * if statement
    	
      The if statement is the simplest form. It takes a condition and evaluates to either True or False.
    	If the condition is True, then the True block of code will be executed, and if the condition is False, 
      then the block of code is skipped, and The controller moves to the next line.
    	
      Syntax of Simple If:
        if condition:
          statement 1
          statement 2
          statement n.
			
      Practice: let’s take a number as an input from a user and then calculate the square of a number if it’s greater than 5.
      	number = input("Enter a number ")
        number = int(number)
        if number > 5:
        print("The square of the entered number is: ", number * number)

    * if ... else statement
    
    	The if ... else statement checks the condition and executes the if block of code when the condition is True, 
      and if the condition is False, it will execute the else block of code.
      
      Syntax of if ... else:
        if condition:
             statement 1
        else:
             statement 2 
            
      Practice: let’s take a number as an input from a user and then check if it is a positive number
        number = input("Enter a number ")
        number = int(number)
        if number > 0:
            print("It is a positive number ")
        else:
            print("It is a negative number or 0 ")
            
    * if ... elif ... else
  	
    	In Python, the if ... elif ... else condition statement has an elif blocks to chain multiple conditions one after another. 
      This is useful when you need to check multiple conditions.
      
      Syntax of if ... elif ... else:

        if condition1:

             statement 1

        elif condition2:

             statement 2

        else:

             statement 3
			
      Practice: let’s take a number as an input from a user and then check if it is a positive number, negative or zero
        number = input("Enter a number ")
        number = int(number)
        if number > 0:
            print("The number you entered is positive ")
        elif number ==0:
            print("The number you entered is  zero")
        else:
            print("The number you entered is negative")
          
	* try / except ( This is used to handle run time error )
    
    	Try and Except statement is used to handle  errors within our code in Python.
      The try block is used to check some code for errors i.e the code inside the try block will execute when there is no error in the program.
			Whereas the code inside the except block will execute whenever the program encounters some errors in the preceding try block.
      
      Syntax of try / except:
        try:
            # Some Code
        except:
            # Executed if error is in the try block
			
      Practice: let’s take a number as an input from a user and then check if it is a positive number, negative,  zero, or catch an input error
        number = input("Enter a number ")
        try:
            number = int(number)
            if number > 0:
                print("The number you entered is positive ")
            elif number ==0:
                print("The number you entered is  zero")
            else:
                print("The number you entered is negative")
        except:
            print("You did not enter a number !!")

 project:
          
  1- Write a code that will take a string from user , and display how many characters are in that string
  2- Write a code that will take a string from user and if the string has less than 4 charactars, it should display "invalid entry" and 
    	if the characters number in the string is more that 4 , it should display "valid entry"
  3- Write a code that will take email address as input and check if @ is in the email receive to tell the user if the email is valid or not.    
  4- Username and password of an application is admin. Write a code that takes two inputs from user username and password and tell the user 
    	 "wrong username or password" if the username and password entered is not admin; and if it is admin and admin, it display "successfully login"
  5- Write a program to take users zip code and check if the input data was a digit number with 5 digits. 
      	(a good zip code has 5 digits) if it is good , display "your entry is valid" if not , display "please enter a valid entry"
  6- Write a script that will take two integers and give the sum of those two int. 
  7- write a script that will take user's age and tell if there are eligible to vote or not. 
  8- Write a program that will take values in kgs from user and convert it into pounds.        
  9- Write a python script that will convert  kg to lbs. it take kg from user and convert it to lbs.
          
                        
 
#List []
Lists are ordered collections of items.
Lists can contain items of different types.
Lists are mutable, meaning their elements can be changed after they are created.
Lists are defined using square brackets [].          
                        
my_list = ['terraform','aws','linux','devops', 2, 5.6, True, 2, 1, 1]

# we can also access element in the list with thier index which make slicing possible          
#List Methods .append() , .pop(), .clear(), .copy() , len(), extend(), insert(), .count(), .sort(), 

          
#Tuple () Tuples in Python are immutable sequences, which means their elements cannot be changed or modified after the tuple is created

my_tuple=(2,4,6,True)
t=(2,4,6,True)          
#tuple methods .count(), index(), len()

#set {} Sets in Python are unordered collections of unique elements
my_set={2 ,2 ,3 ,4 ,5 , True}
m={2 ,2 ,3 ,4 ,5 , True}          

#set methods add(), update(), remove(), pop(), clear(), discard(), len()

#Dictionary {} Dictionaries in Python are a collection of key-value pairs. They are highly flexible and WIDELY used for data manipulation
d={'name':'Serge', 'age': 30, 'profession': 'DevOps Engineer', 'courses':['aws','linux','terraform']}

#Dict methods .clear(), .copy(), .get() items(), keys(), values(), pop(), popitem(), update(dict), del(dict[key])          

          
 ***Python Loops

Python has two primitive loop commands:
                        

for loops                         
while loops

**Python For Loops
A for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string).
This is less like the for keyword in other programming languages, and works more like an iterator method as found in other object-orientated programming languages.
With the for loop we can execute a set of statements, once for each item in a list, tuple, set etc.                        
                        
**The while Loop
With the while loop we can execute a set of statements as long as a condition is true.                        

**The break Statement
With the break statement we can stop the loop even if the while condition is true: 
                        
**The continue Statement
With the continue statement we can stop the current iteration, and continue with the next: 
 

- Functions
    
    In Python, a function is a block of organized, reusable code that is used to perform a specific task.
    Functions help break our program into smaller and modular chunks. 
    As our program grows larger and larger, functions make it more organized and manageable.
    
    * how to define a function
    
    Here are simple rules for a user to define a function:
        1- Function blocks begin with the keyword def followed by the function name and parentheses ( ( ) ).
        2- Add parameters to the function: they should be within the parentheses of the function. End your line with a colon ( : ).
        3- Add statements that the functions should execute.
        4- End your function with a return statement if the function should output something. Without the return statement, your function will return an object None.
		
  	Syntax of a User-defined function:
        def function_name(parameters):
          " " "docstring" " "
          statement(s)
          return expression

        docstring is an optional documentation string to describe what the function does.
    	Examples: 	def  welcome():
   								print('Hi !! Welcome to our python course')
            
            		def  hello():
   									print('Hello everyone')
    * how to call a function 
    
      To call a function, we use the function name followed by parenthesis, and optionally arguments inside parentheses.
        Example: hello()        
          
    * Functions arguments
    
    	When defining a function, we can take variables as input of that function. Those variables are called "parameters" of the function.
				Example: def sum(a,b):	=>  a and b are parameters of sum() function

    	When calling the function, we specify values of those variables. Those values are called arguments of the function.
				Example: sum(10,6) 		=> 10 and 6 are arguments of sum() function
      
  	- Return value and pass statement
		
      * Return value
        Sometimes, we use a return value when we want to use the result of a function. 
        To let a function return a value, use the return statement.
          Example: 	def square(x):
                      return x * x

      * Pass statement

        Function definition can not be empty, but if for some reason you don’t want to add content, use the pass statement to avoid an error.
          Example: 	def myfunction():
                      pass 
 
          '''Functions in Python are reusable blocks of code designed to perform a specific task. They help in organizing code,
making it more readable,
and reducing redundancy.
'''


'''To define a function in Python, you use the def keyword, followed by the function name, 
parentheses (), and a colon :. '''

def function_name(parameters):
    """docstring"""
    statement(s)

def greet():
    """This function prints a greeting message."""
    print("Hello, welcome to Python functions!")

# Calling the function
greet()


#A function can be likened to a machine or appliance, like a washing machine.

def wash_clothes(clothes, detergent, settings):
    # Simulate the washing process
    return f"Washed {clothes} with {detergent} on {settings} settings"

# Calling the function
print(wash_clothes("shirts and pants", "liquid detergent", "normal"))
# Output: Washed shirts and pants with liquid detergent on normal settings
          
          
          
          
          '''Parameters and Arguments
Functions can take parameters (also called arguments) that provide inputs to the function.

Types of Parameters
Positional Parameters: Parameters that must be passed in the correct order.
Keyword Parameters: Parameters that are passed with a key=value syntax.
Default Parameters: Parameters that have default values if no value is provided.
Arbitrary Arguments: Parameters that can take any number of arguments
using *args for positional arguments and **kwargs for keyword arguments.
'''

def add(a, b):
    """This function returns the sum of two numbers."""
    return a + b

# Calling the function with positional arguments
print(add(5, 3))  # Output: 8

def greet(name, message="Hello"):
    """This function prints a greeting message with a name."""
    print(f"{message}, {name}!")

# Calling the function with a keyword argument
greet(name="Alice")  # Output: Hello, Alice!
greet(message="hii",name="anish")  # Output: hii, anish!

          
          '''The return statement is used to exit a function and go back to the place
where it was called.
Optionally, a function can return a value or a set of values.'''

def multiply(a, b):
    """This function returns the product of two numbers."""
    return a * b

# Calling the function and storing the result in a variable
result = multiply(4, 5)
print(result)  # Output: 20

def calculate_area(radius):
    """This function returns the area and circumference of a circle."""
    area = 3.14 * radius ** 2
    circumference = 2 * 3.14 * radius
    return area, circumference

# Calling the function and unpacking the returned values
area, circumference = calculate_area(5)
print(f"Area: {area}, Circumference: {circumference}")       
          
  - Modules: 
      
    	A module is a file containing Python definitions and statements.
			We can also define a module as a file containing a set of functions you want to include in your application.
      In Python, Modules are simply files with the ".py" extension containing Python code that can be imported inside another Python Program.
      
      * import a module using import statement:
        	
          We can import the functions, classes defined in a module to another module using the import statement in some other Python source file.
          Syntax:  import module_name
            
      * import from a module using from ... import statement
        	
          The from ... import statement allows you to import specific functions/variables from a module instead of importing everything.
					Syntax:  from module_name import function_name/variable_name
            
      * Using from ... import * statement
        
          The from ... import * statement allows you to import all the names from a module to a current namespace.
          The use of * has its advantages and disadvantages. If you know exactly what you will be needed from the module, it is not recommended to use *, else do so.
          Syntax:  from module_name import * 
            
      * Renaming the imported module 

          You can rename the module you are importing, which can be useful in cases when you want to give a more meaningful name to the module or the module name is too large to use repeatedly. 
          You can use the "as" keyword to rename it. 
          Syntax:  import module_name as module_rename          
   
  * Built-in modules
    	
        Built-in modules are written in C and integrated with the Python shell. 
        Each built-in module contains resources for certain system-specific functionalities such as OS management, disk IO, etc.
        To display a list of all available modules, use the following command in Visual studio code terminal:
          
          >> python3
          >> help('modules')
        
        *	OS module
        	
          It is possible to automatically perform many operating system tasks. The OS module in Python provides functions for 
          creating and removing a directory (folder), fetching its contents, changing and identifying the current directory, etc.
					You first need to import the os module to interact with the underlying operating system. 
          	
            Practice: From the python interpreter console:
                
              >> import os
              # get the current working directory
              >> os.getcwd()

              # create a new directory
              >> os.mkdir('file path') # example for my side 'file path'='C:\\Users\\Reyel\\Utrains\\MyPythonProject'
							
              # to create a directory recursively
              >> os.mkdirs("path")
              
              # remove or delete a file not a directory
              >> os.remove("path")
              
              # remove or delete an empty directory
              >> os.rmdir('file path') # example for my side 'file path'='C:\\Users\\Reyel\\Utrains\\MyPythonProject'
              
              # get the list of all files and directories in the specified directory
              >> os.listdir(path)
              
              # display block devices and partition information
              >> os.system('lsblk')
              
              # Return True if this entry is a file or a symbolic link pointing to a file        
              >> os.path.isfile("path")
              # display the number of cpu
              >> os.cpu_count() or os.system('nproc')
          Exercise:
            
            1- Write a program to inventory the linux system

              import os

              _numofCpu = os.cpu_count()
              # display the number of cpu
              print(_numofCpu)
              # display a detailed report on the system's memory usage
              os.system('free -m')
              # list informations about all available or the specified block devices
              
        
        * Sys module
        		
          The sys module provides functions and variables used to manipulate different parts of the Python runtime environment.
          
          	Practice: From the python interpreter console:
                
                	>> import sys
                  # Print the platform information
                  >> sys.platform
                  
                  # get command line argument 
                  sys.argvs()
                  
                  # display a string containing the version number of the current Python interpreter
                  >> sys.version
                  #Exits from Python. If an argument is given, it is passed as the exit status 
                   (which can be an integer or a string). A value of 0 typically indicates a successful termination, while non-zero values indicate an error.
Example:           >> sys.exit([arg]) 
                   >>sys.exit(0) or sys.exit("Error message")
                  
                  
        * Platform module
        
        	The Platform module is used to retrieve as much possible information about the platform on which the program is being currently executed.	
        	
          Practice: From the python interpreter console:
        						
                		>> import platform
                  
                  	# display platform architecture 
                    >> platform.architecture()
                    
                    # display platform information
                    >> platform.platform()
                    
                    # display OS naame
        						>> platform.system()
                    
                    # display the python version
                    >> platform.python_version()
        
				* Time
        	
          The time module offers functions for time-related operations, including measuring time, pausing program execution, and formatting time values.
          
          Practice: From the python interpreter console:
              
                  >> import time
                  # Get the current time in seconds since January 1, 1970 (known as the epoch)
                  >> current_time = time.time()
                  >> print("Current time:", current_time)

                  # Convert a time value to a readable string
                  >> readable_time = time.ctime(current_time)
                  >> print("Readable time:", readable_time)
                  # Pause the program for 5 seconds
                  >> time.sleep(5)
                  
     		* Subprocess module
        	
          The subprocess module in Python provides functionalities for spawning new processes, connecting to their input/output/error pipes, 
          and obtaining their return codes.
        	
          Practice: 
            	import subprocess

              # Run a shell command and capture its output
              result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE,stderr=subprocess.PIPE)
              print(result.stdout.decode())
               
              # Run a shell command with arguments
              subprocess.call(['echo', 'Hello, World!'])

              # Open a subprocess with pipes for input, output, and error
              process = subprocess.Popen(['ls', '-l'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
              output, error = process.communicate()
              print("Output:", output.decode())
              print("Error:", error.decode())
              
        * Paramiko
        
          The paramiko module allows you to create SSH connections, securely transfer files, execute commands on remote servers, and manage SSH keys.

					Practice:
          
              import paramiko

              # SSH connection parameters
              hostname = "remote_server_ip_or_hostname"
              port = 22
              username = "your_username"
              password = "your_password"

              # Create an SSH client
              ssh_client = paramiko.SSHClient()

              # Automatically add the server's host key (this is insecure and should be handled properly in production)
              ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

              try:
                  # Connect to the remote server
                  ssh_client.connect(hostname=hostname, port=port, username=username, password=password)
                  '''private_key = paramiko.RSAKey.from_private_key_file("/path/to/private/key")
									ssh_client.connect(hostname=hostname, port=port, username=username, pkey=private_key)
									'''
                  # Execute a command on the remote server
                  stdin, stdout, stderr = ssh_client.exec_command("ls -l")

                  # Print the command output
                  print("Command Output:")
                  print(stdout.read().decode())

              except Exception as e:
                  print(f"Error: {e}")
              finally:
                  # Close the SSH connection
                  ssh_client.close()

              ## Create an SFTP client session
              sftp = ssh_client.open_sftp()

              ## Upload the local file to the remote server
              sftp.put(local_file_path, remote_file_path)

    
                
        * Random module
						
          The random module is a built-in module to generate the pseudo-random variables. 
          It can be used perform some action randomly such as to get a random number, selecting a random elements from a list, shuffle elements randomly, etc.
					
          Practice: From the python interpreter console:

                >> import random
                # get a random float number between 0.0 to 1.0 by default
                >> random.random()

                # get a random integer between two specified integers.
                >> random.randint(1,20)

                # randomly reorders the elements in a list.
                >> numbers=[12,23,45,67,65,43]
                >> random.shuffle(numbers)
                >> print(numbers)
                
                # Choose a number or string from a given list
                >> number=random.choice(numbers)
                >> print(number)
                
                
        * Range function (not module)
        
        		The range function is not part of a specific module. It is actually a built-in function that allows you to generate a sequence of numbers.
          	
            Syntax: range(start, stop, step)
              
            Practice: From the python interpreter console:

               #  convert a range object to a list using the list function
                >> numbers = list(range(1, 10, 2))
                >> print(numbers)
                
                # for loops to iterate over a sequence of numbers
                >> for i in range(5):
                  		print(i)
        
        
        Here are some useful external modules for python.(Requests, python-jenkins, Boto3, Selenium)

        Note: Before using these modules, make sure you have install it with the pip command as follows: pip3 install module_name
  ***A virtual environment in Python is a self-contained directory that contains a Python 
    interpreter and a set of installed packages. The main reasons for using virtual environments are:
    Different projects often require different versions of packages, which can lead to 
    conflicts if they are installed globally. Virtual environments allow you to create isolated environments for each project, ensuring that dependencies are kept separate.
         
          - Create or activate a virtual env for python
  
		# create and activate a virtual env for python
  	pip3 install virtualenv
    virtualenv env		#create the virtual env if not yet created
    source env/Scripts/activate  ( for windows)
    source env/bin/activate  (for mac or linux)

          
          
***Project: 
          1- AT work,we have many jenkins servers and there is a need to inventory those servers. it is a 
          Stidious task to manually got in jenkins and count the jobs, and the vcs link associated
          to each of them. go ahead and propose a solution for this.
          jenkins url: 'http://44.197.113.73:8080'
          jenkins username: 'devops'
          jenkins password: 'devops'
          
*** 
  2- write a script that will be used to inventory the servers ( windows and linux)
  it should display system information like the os version, the number of cpu, the memory size, the python version installed.
          
*** 
    3- in the CI/CD pipeline, one of the stages is to change the deployment version in the helm chart.
    the file is called chart.yaml and below is the content
apiVersion: v2
name: geo-app
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: "1.16.0"
          
sol:
import shutil 

shutil.copy('projects/chart.yaml', 'projects/chart_bk.yaml')

def chartModif(chart_version):
    with open("projects/chart.yaml", 'r') as f:
        content = f.readlines()
        
    with open("projects/chart.yaml" , 'w')  as f1:
        for line in content:
            if 'version' in line:
                f1.write(f'version: {chart_version}\n')
            else:
                f1.write(line)
     
chartModif('3.0')    
           
         
4- We have 5 applications that we support and need a way to petiodically check each endpoint if it is up
provide a solution for this.
          
5- in one stage of the pipeline , we need to check if jfrog server is up and can accept credentials.
Any artifact push. please write a python script to do this.          
          
sol:
          
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time 

driver = webdriver.Chrome()
#time.sleep(2)
driver.maximize_window()
#time.sleep(2)
driver.get("http://54.197.99.224:8080")

username = driver.find_element(by=By.NAME, value='j_username')
password = driver.find_element(by=By.NAME, value='j_password')
username.send_keys('devops')
password.send_keys('devops')
submit_button = driver.find_element(by=By.NAME, value='Submit')
submit_button.click()
time.sleep(4)
          
          
          
          
we want a way to automatically change the version from 0.1.0 to 2.0 before the deployment.

%%%% 
***** jenkins_api.py
          
import jenkins
import csv

def listJenkinJobs(url,username,password):
    server = jenkins.Jenkins(url, username=username, password=password)

    _jobs = server.get_all_jobs()
    job_list=[]
    for job in _jobs:
        job_name= job.get('name')
        job_url= job.get('url')
        job_color=job.get('color')
        job_list.append([job_name,job_url,job_color])
    return job_list
    
def storeJenkinsInfo(file_name,job_list):
    with open(file_name, 'w',newline='') as f:
        writer = csv.writer(f)
        HEADER=['Job_name', 'Job_url', 'Job_color']
        writer.writerow(HEADER)
        for j in job_list:
            writer.writerow(j)
            
jobs=listJenkinJobs('http://44.197.113.73:8080','devops','devops') 
storeJenkinsInfo("week22.csv",jobs)
          
**** second version
import jenkins
import csv

def listJenkinJobs(url,username,password):
    server = jenkins.Jenkins(url, username=username, password=password)

    _jobs = server.get_all_jobs()
    job_list=[]
    for job in _jobs:
        job_name= job.get('name')
        job_url= job.get('url')
        job_color=job.get('color')
        job_list.append([job_name,job_url,job_color])
    return job_list
    
def storeJenkinsInfo(file_name,job_list):
    with open(file_name, 'w',newline='') as f:
        writer = csv.writer(f)
        HEADER=['Job_name', 'Job_url', 'Job_color']
        writer.writerow(HEADER)
        for j in job_list:
            writer.writerow(j)
            
def main():
    jobs=listJenkinJobs('http://44.197.113.73:8080','devops','devops') 
    storeJenkinsInfo("week22.csv",jobs)
    print("File generated successfully")          

if __name__=="___main___":    
    main()   
 
          
 **** aws_api.py
          
import boto3 

_ec2 = boto3.client('ec2',region_name='us-east-1')
response = _ec2.describe_instances()

instance_list=[]
for item in response['Reservations']:
    instance_list.append(item['Instances'][0]['InstanceId'])
print(instance_list) 

*** aws_api.py
          
import boto3 

_ec2 = boto3.client('ec2',region_name='us-east-1')
response = _ec2.describe_instances()

instance_list=[]
for item in response['Reservations']:
    instance_list.append(item['Instances'][0]['InstanceId'])
try:
    _ec2.start_instances(InstanceIds=instance_list)
except Exception as e:
    print(f"Error: {e}")          
          
 
*********Projects: 
 1-  FinOps (Financial Operations) is a concept which involves  various aspects such as cost
optimization, budget tracking, and financial analysis, particularly in cloud environments
In the case of aws, we have boto3 python mopdule that can be used to collect valuable data about 
we need to write some script to help in this regards.
- collect info about ec2 instances like instance id, instance type key_name and store that in a spread sheet          
- collect info about ebs volume with tag={env:dev}     
- collect info about public s3 buckets if any
- Users with access and secret key attach to thier account.
- some of these script needs to be scheduled to run once a week and send email to corresponding team for correct actions.
Go ahead and write bellow functions:
- a functions that will take region name and tag{env:value} ( value can be dev,qa,prod)  as argument  and then give all the ec2 ids
- a function that will give all the ec2 that are stopped as a list of instance id          
- a function that will take a list of instance ids and start them 
- a function that will take a list of instance ids and stop them  
- a function that will check if an ec2 doesnt have the tag env, that instance will be stopped.
- a function that will list all bucket with public access 
- a function that will give the list of users that have access key created.          
- a script that will shutdown dev server at 5pm and bring them back up at 6 am           

 
*** Boto3
* Go to boto3 documentation to understand how to interact with some aws services ( ec2, s3, iam, volumes, ses, route53)
https://boto3.amazonaws.com/v1/documentation/api/latest/index.html  
          
***** terraform code 

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "4.61.0"
    }
  }
}    
 provider "aws" {
        region = "us-east-1"  
      }

resource "aws_instance" "dev_instances" {
  ami           = "ami-01eccbf80522b562b"  
  instance_type = "t2.micro"      

  tags = {
    Name = "dev-server"
    env  = "dev"
  }
}
resource "aws_instance" "prod_instances" {
  ami           = "ami-01eccbf80522b562b"  
  instance_type = "t2.micro"      

  tags = {
    Name = "prod-server"
    env  = "prod"
  }
}          

resource "aws_instance" "qa_instance" {
  ami           = "ami-01eccbf80522b562b"  
  instance_type = "t2.micro"      

  tags = {
    Name = "qa-server"
    env  = "qa"
  }
}              
          
**** Filters 

dev=[{'Name': 'tag:env', 'Values': ['dev']}]
qa=[{'Name': 'tag:env', 'Values': ['qa']}]
prod=[{'Name': 'tag:env', 'Values': ['prod']}] 
Instance-type= [{'Name': 'instance-type', 'Values': ['t2.xlarge'] }]

import boto3

ec2 = boto3.client('ec2', region_name="us-east-1")
#_iam = boto3.client('iam')
#_s3 = boto3.client('s3')
dev_filter = {'Name': 'tag:env', 'Values': ['dev']}
qa_filter = {'Name': 'tag:env', 'Values': ['qa']}
prod_filter = {'Name': 'tag:env', 'Values': ['prod']}
stopped_instance = {'Name': 'instance-state-name' , 'Values' : ['stopped']}
instance_type_filter = {'Name': 'instance-type' , 'Values' : ['t2.micro']}

def listInstances(*args):   # **kwargs
    
    try: 
        response = ec2.describe_instances(Filters=[*args])
        instance_list = []
        for instance in response['Reservations']:
            instance_id = instance['Instances'][0]['InstanceId']
            instance_list.append(instance_id)

        return instance_list
    except Exception as e:
        print(f"ERROR: {e}")

def startInstances(instance_list):
    try:
        ec2.start_instances(InstanceIds=instance_list)
    except Exception as e:
        print(f"Error: {e}")

def stopInstance(instance_list):
    try:
        ec2.stop_instances(InstanceIds=instance_list)
    except Exception as e:
        print(f"Error: {e}")




*** start_instance_lambda.py
          
import boto3

ec2 = boto3.client('ec2', region_name="us-east-1")
dev_filter = {'Name': 'tag:env', 'Values': ['dev']}

def listInstances():   # **kwargs
    
    """ This listing the ec2 instances"""
    try: 
        response = ec2.describe_instances(Filters=[dev_filter])
        instance_list = []
        for instance in response['Reservations']:
            instance_id = instance['Instances'][0]['InstanceId']
            instance_list.append(instance_id)

        return instance_list
    except Exception as e:
        print(f"ERROR: {e}")

def startInstances(instance_list):
    try:
        ec2.start_instances(InstanceIds=instance_list)
        print("Instance started successfully...")
    except Exception as e:
        print(f"Error: {e}")


s = listInstances()
startInstances(s)
          
          
%%%%%%% jull ci/cd jenkinsfile
          
pipeline{
    agent any 
    tools {
        maven 'M2_HOME'
    }
    environment {
      AWS_REGION = 'us-east-1'
      SCANNER_HOME= tool 'sonar'
      ARTIFACTORY_URL=  'http://ec2-3-84-197-172.compute-1.amazonaws.com:8081/artifactory'
      REPO = 'geolocation'
      APP_NAME = 'geoapp'
      APP_OWNER = 'cloud_team'
      DOCKER_REPO_NAME = "${APP_NAME}"
      REPO_URL     = 'public.ecr.aws/g0j7o9l5'
      DOCKER_REPO = "${REPO_URL}/${DOCKER_REPO_NAME}"
      BRANCH_NAME= 'main'
      GIT_CRED = 'Github_ID'
      PROJECT_URL = 'https://github.com/kserge2001/full-cicd-code'

    }
    stages{
        stage('Git Checkout'){
            steps{
                git branch: "${BRANCH_NAME}", credentialsId: "${GIT_CRED}", \
                url: "${PROJECT_URL}"
            }
        }
        
        stage('Unit Test'){
            steps{
                sh 'mvn clean'
                sh 'mvn compile '
                sh 'mvn test'
            }
        }
        
        stage('Sonarqube Scan'){
            steps{
                withSonarQubeEnv(credentialsId: 'sonarqube_ID', \
                installationName: 'Sonarqube') {
              sh ''' $SCANNER_HOME/bin/sonar-scanner -Dsonar.projectName=geo-app -Dsonar.projectKey=geo-app \
                   -Dsonar.java.binaries=. '''
}
            }
        }
        stage('Quality Gate Check'){
            steps{
              script{
                 waitForQualityGate abortPipeline: false, credentialsId: 'sonarqube_ID' 
              }
            }
        }
        stage('Trivy Scan'){
            steps{
                 sh "trivy fs --format table -o maven_dependency.html ."
            }
        }
        
        stage('Code Package'){
            steps{
                sh 'mvn package'
            }
        }
        
        stage('Upload Jar to Jfrog'){
            steps{
                 withCredentials([usernamePassword(credentialsId: 'j_frog-cred', \
                 usernameVariable: 'ARTIFACTORY_USER', passwordVariable: 'ARTIFACTORY_PASSWORD')]) {
                    script {
                        // Define the artifact path and target location
                        def artifactPath = 'target/*.jar'
                        def targetPath = "release_${BUILD_ID}.jar"

                        // Upload the artifact using curl
                        sh """
                            curl -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \
                                 -T ${artifactPath} \
                                 ${ARTIFACTORY_URL}/${REPO}/${targetPath}
                        """
            }
        }
    }

}

    stage('Docker image Build'){
        steps{
            script{
           sh "docker build --no-cache -t ${DOCKER_REPO}:latest ."
           sh "docker build --no-cache -t ${DOCKER_REPO}:${BUILD_ID} ."
            }
           
        }
    }

    stage('Scan Docker Image'){
        steps{
          sh """trivy image --format table -o docker_image_report.html ${DOCKER_REPO}/${APP_NAME}:${BUILD_ID}"""
  
        }
    }
  
    stage('Push Image To Registry'){
        steps{
          script{
        //def ecr_passwrd=sh(script: "aws ecr-public get-login-password --region 'us-east-1'")
         //sh "docker login --username AWS --password ${ecr_passwrd} public.ecr.aws/g0j7o9l5"   
        sh "aws ecr-public get-login-password --region 'us-east-1' | docker login --username AWS --password-stdin public.ecr.aws/g0j7o9l5"
        sh "docker push ${DOCKER_REPO}:latest "
        sh "docker push ${DOCKER_REPO}:${BUILD_ID} "
        }
    }
    }
   
    stage('Chart Version Update'){
        steps{
         sh "python3 setup_script/charUpdate.py ${BUILD_ID}" 
           
        }
    }
    stage('Package Helm'){
        steps{
            sh 'helm package geo-app'
        }
    }
    stage('Upload helm package to Jfrog'){
            steps{
                 withCredentials([usernamePassword(credentialsId: 'j_frog-cred', \
                 usernameVariable: 'ARTIFACTORY_USER', passwordVariable: 'ARTIFACTORY_PASSWORD')]) {
                    script {
                        // Define the artifact path and target location
                        def artifactPath = 'geo-app-${BUILD_ID}.tgz'
                        def targetPath = "heml/geo-app-${BUILD_ID}.tgz"

                        // Upload the artifact using curl
                        sh """
                            curl -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \
                                 -T ${artifactPath} \
                                 ${ARTIFACTORY_URL}/${REPO}/${targetPath}
                        """
            }
        }
    }

}
    }
    post{
        always{
            echo 'always'
        }
        failure{
            echo 'failed'
        }
        success{
            echo 'success'
        }
    }
}
          
*** devops repo
          
https://github.com/utrains/DEVOPS-UTRAINS.git          

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Review strategy ( Monday -- Sunday)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
*** Linux ==> one day ==> Monday  #### Sys Admin
    ** Networking:
       - ip address
       - mac address
       - osi model
       - hub , switch
       - Dns ( what happened when we type goolge.com on the browser)
    ** Linux Sys admin
      - user and groups
      - file and directory permission
      - install package or application ( apache, jenkins, sonarqube , jfrog) on linux server
      - system inventory 
    ** Troubleshooting:
          - hardware 
          - Os level  ( read errors, check logs, Monitoring tool)
          - application ( daemon? , port ? database? networking ? ==> get corresponding team involve )  )
    ** Scripting ( shell script) 
      - the first line is shell script
      - return code
      - how to close if statement
      * Example:
       - system inventory script
       - package install script
       - jenkins, sonarqube,  splunk  install scripts
    ** Docker
     - what is docker?
     - why?
     - what is an image ?
     - describe some docker command and usage.
     - difference between vm, k8s and docker
     - able to build images.
     - best practice around Dockerfile 
     - cons
      
   ***** Aws , Terraform ==>  Tuesday Wednesday      
          
    ** AWS:
     - what is the cloud compute ?
     - what is AWS?
     - why care about the cloud                    
     - cons
     - type of cloud solution: private, public, hybrid, multi cloud                   
     - aws services:
       - Compute: lightsail, ec2, beanstalk , lambda
       - Storage: s3, ebs, efs, 
       - Database: rds , dynamoDB
       - Governance: cloudwatch, cloudtrail                  
       - Network: route53, vpc, subnet, natgateway, loadbalancer    
       - Content Delivery Network: cloudfront   
       - Application Integration: sns   
       - Security:  Iam, acm, waf , security group , kms
       - Devops: codecommit, codebuild, codepipeline, codedeploy                 
       - Containers: ecr, fargate, ecs, eks
                        
  *** Project:
       - create many lambda function to execute a python script.( script to check users with access key) schedule it thru rules in cloudwatch to execute every week                
       - python scripts: script to stop and start ec2 instance, list ec2 with no tag environment 
       - participate in aws migration project. and my role was to write terraform code.
       - POC on serverless architecture in aws using services like lambda, api gateway,DynamoDB.                 
       - talk about some services that you created or manage manually. 
       - I also use awsw devops service to create fully automated CI/CD pipeline.                 
                        
   *** Terraform:
       - what is IaC?
       - why?
       - what terraform?
       - building blocks ( provider, resource, variables, output, import, module, data,) 
       - what is a module and why?
       - how to manage different environment (dev , qa , stagging, uat, prod)                 
       - what is the statefile ( where you do you keep it)
       - null_resource
       - provider?
       - how manage sensitive data
       - provisioner ( local-excec, remote-excec, file) 
       - infr cost 
       - terraform entreprise
       - terraform best practice
       - terraform commands.
       - terraform cons
                        
    *** Project                     
       - participate in aws migration project. and my role was to write terraform code.
       - participate in writing terraform code for a multi tiers app in aws.
       - Worked on configuring our statefile in a remote location ( s3) and dynamodb table to lock it.
       - resolved ticket that are for existing code modification.
                        
***** DevOps  ==> Thursday - Sunday ***********
  
    - The SDLC lifecycle ?
    - why DevOps?
    - Philosophy of DevOps
    - shift left                    
    - Who is a good DevOps engineer ? (Listen, analyse, communicate, automate, test, test ,test before production, leadership, security first, solution oriented)
    - Tools:
      - vcs: github
      - test Tools ( code analysis ): trivy, sonarqube, maven , jUnit, pytest  
      - CI tools: jenkins ( pipelines ==> jenkinsfile == > Declarative pipeline)
      - CD tools: eks, ansible
      - configuration management tool: ansible
      - monitoring tools: splunk, prometeus and grafana
      - Artifactory tools: ecr, s3, jfrog
      - incident ticket tool (Production): service now
      - where do you feel comfortable?
      - Able to describe the full CI/CD process ( you dont have to set it up)
      - Security: 
          dev:
          qa, uat, nonprod:
          ci: 
            jenkins:
          cd:
            ansible:
            eks: 
          prod: 
            code:
            rollback
            hostfix
            monitoring
            
    Projects:
      - worked in a project where we needed to deploy containerize app, and my job was to integrate jenkins with ecr
      - worked on a project to help developer automate the maven built process.( Integrated jenkins with maven)
      - Setting up webhooks on few jobs for automated build trigger.
      - Integrated jenkins with sonarqube for code review,code test.
      - work with dev , qa,uat team and prod to implement various solutions. (Data base migration, lens implementation,
        prometheus implementation, eks implementation.)
      - Migrate jenkins jobs to jenkins pipeline job
      - add different stages to jobs with built in test cases from developers
      - gitops ( treating  terraform code as dev code )
                              
 *** DevOps strategy 
                        
 https://dataservicegroup.atlassian.net/wiki/spaces/DT/pages/edit-v2/2303229992       